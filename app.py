################ Lancer l'application avec : ###########################
# streamlit run app.py
########################################################################
import os
import sqlite3
import time
import json
from typing import Callable, Dict, List, Optional

import pandas as pd
import pymongo
from pymongo.errors import PyMongoError
import streamlit as st
from groq import Groq
from dotenv import load_dotenv, set_key


DOSSIER_DATA = "data"
DOSSIER_SQLITE = "sqlite"
DB_FILE = os.path.join(DOSSIER_DATA, DOSSIER_SQLITE, "db", "paris2055.sqlite")
DOSSIER_CSV = "./data/sqlite/resultats_requetes_sqlite/"
DOSSIER_JSON = os.path.join(DOSSIER_DATA, "mongodb", "collections")
DOSSIER_MONGO_CSV = os.path.join(
    DOSSIER_DATA,
    "mongodb",
    "resultats_requetes_mongodb",
)

MONGO_URI = "mongodb://127.0.0.1:27017/"
MONGO_DB_NAME = "Paris2055"

load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")

SCHEMA_CONTEXT = """
Tu es un expert MongoDB et Python. Ton but est de traduire une question naturelle en requ√™te d'agr√©gation MongoDB.
Voici le sch√©ma de la base de donn√©es 'Paris2055' :

1. Collection 'lignes' :
    - Documents : { "id_ligne": int, "nom_ligne": str, "type": str (Bus, Tramway...), "frequentation_moyenne": int, "arrets": [...], "trafic": [{ "retard_minutes": int, "incidents": [...] }] }

2. Collection 'capteurs' :
    - Documents : { "id_capteur": int, "type_capteur": str (Bruit, CO2, Temperature), "mesures": [{ "valeur": float, "horodatage": date }], "arret": { "nom_ligne": str, "nom_arret": str } }

3. Collection 'quartiers' :
    - Documents : { "nom": str, "arrets": [...] }

R√àGLES STRICTES DE G√âN√âRATION :
1. Tu dois r√©pondre UNIQUEMENT un objet JSON valide au format :
    {
      "collection": "nom_collection",
      "pipeline": [ ... ]
    }

2. **R√àGLE D'AFFICHAGE (PROJECTION)** : 
    Tu dois OBLIGATOIREMENT ajouter une √©tape `"$project"` √† la fin du pipeline pour nettoyer le r√©sultat.
    - Garde UNIQUEMENT le nom de l'entit√© (ex: `nom_ligne`, `nom`, ou `arret.nom_ligne`) et la valeur calcul√©e/demand√©e.
    - SUPPRIME syst√©matiquement `_id` (`"_id": 0`).
    - SUPPRIME syst√©matiquement les listes lourdes (`arrets`, `trafic`, `mesures`) sauf si l'utilisateur demande explicitement de les voir.

3. **R√àGLE D'IMPOSSIBILIT√â** : 
    Si la question de l'utilisateur n'a ABSOLUMENT rien √† voir avec le sch√©ma de la base de donn√©es, ou s'il est techniquement impossible d'y r√©pondre avec une requ√™te MongoDB (par exemple, une question de philosophie ou une requ√™te impossible m√™me avec l'agr√©gation), tu dois retourner le JSON suivant, sans changer la collection :
    {
      "collection": "lignes",
      "pipeline": []
    }
"""

MIGRATION_LOG_PLACEHOLDER: Optional[st.delta_generator.DeltaGenerator] = None
MAX_LOG_LINES = 300


# =====================================================================
# UTILITAIRES GENERAUX
# =====================================================================

def enregistrer_resultats_csv(
    lien_dossier: str,
    nom_fichier: str,
    dataframe: pd.DataFrame,
) -> None:
    """
    Enregistre un DataFrame au format CSV dans le dossier indiqu√©.

    Param√®tres
    ----------
    lien_dossier : str
        Chemin du dossier de sortie.
    nom_fichier : str
        Nom du fichier CSV √† cr√©er.
    dataframe : pandas.DataFrame
        Donn√©es √† exporter.

    Retour
    ------
    None
        Le fichier est √©crit sur le disque, une exception est lev√©e en cas d'erreur.
    """
    os.makedirs(lien_dossier, exist_ok=True)
    full_path = os.path.join(lien_dossier, nom_fichier)
    dataframe.to_csv(full_path, index=False, encoding="utf-8-sig")


def aggregate_to_df(collection, pipeline: List[Dict]) -> pd.DataFrame:
    """
    Ex√©cute un pipeline d'agr√©gation MongoDB et retourne le r√©sultat en DataFrame.

    Param√®tres
    ----------
    collection :
        Collection MongoDB sur laquelle ex√©cuter l'agr√©gation.
    pipeline : list[dict]
        Pipeline d'agr√©gation MongoDB.

    Retour
    ------
    pandas.DataFrame
        R√©sultat de l'agr√©gation, vide si aucun document n'est retourn√©.
    """
    documents = list(collection.aggregate(pipeline))
    if not documents:
        return pd.DataFrame()
    return pd.DataFrame(documents)


def to_datetime(value) -> Optional[object]:
    """
    Convertit une valeur vers un objet datetime Python si possible.

    Param√®tres
    ----------
    value :
        Valeur initiale (texte ou datetime-like).

    Retour
    ------
    datetime | None
        Objet datetime si la conversion r√©ussit, None sinon.
    """
    if pd.isna(value):
        return None
    try:
        dt = pd.to_datetime(value, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.to_pydatetime()
    except Exception:
        return None


def parse_geojson_geometry(geojson_str: Optional[str]) -> Optional[Dict]:
    """
    Extrait la g√©om√©trie (type / coordinates) d'une cha√Æne repr√©sentant un objet GeoJSON.

    Param√®tres
    ----------
    geojson_str : str | None
        Cha√Æne JSON repr√©sentant un objet GeoJSON ou une Feature.

    Retour
    ------
    dict | None
        Dictionnaire avec les cl√©s 'type' et 'coordinates', ou None si non exploitable.
    """
    if not isinstance(geojson_str, str):
        return None

    data = geojson_str.strip()
    if not data:
        return None

    try:
        obj = json.loads(data)
    except Exception:
        return None

    # Cas simple : la cha√Æne repr√©sente directement une g√©om√©trie GeoJSON.
    if isinstance(obj, dict):
        if "type" in obj and "coordinates" in obj:
            return {"type": obj["type"], "coordinates": obj["coordinates"]}

        # Cas Feature GeoJSON : on r√©cup√®re la sous-cl√© geometry.
        geometry = obj.get("geometry")
        if isinstance(geometry, dict) and {
            "type",
            "coordinates",
        }.issubset(geometry.keys()):
            return {
                "type": geometry["type"],
                "coordinates": geometry["coordinates"],
            }

    return None


def infer_unite_from_type(type_capteur: Optional[str]) -> Optional[str]:
    """
    D√©duit une unit√© de mesure par d√©faut selon le type de capteur.

    Param√®tres
    ----------
    type_capteur : str | None
        Libell√© du type de capteur (Bruit, CO2, Temp√©rature, etc.).

    Retour
    ------
    str | None
        Unit√© probable (dB, ¬∞C, ppm), ou None si aucune correspondance √©vidente.
    """
    if not isinstance(type_capteur, str):
        return None

    lower_value = type_capteur.lower()
    if "bruit" in lower_value:
        return "dB"
    if "temp" in lower_value:
        return "¬∞C"
    if "co2" in lower_value:
        return "ppm"
    return None


# =====================================================================
# CONNECTIVITE MONGODB ET LOGS
# =====================================================================

def check_connexion_details() -> tuple[bool, bool]:
    """
    V√©rifie l'√©tat du serveur MongoDB et la pr√©sence de la base Paris2055.

    Retour
    ------
    (bool, bool)
        - premier bool√©en : True si le serveur r√©pond au ping.
        - second bool√©en : True si la base MONGO_DB_NAME existe sur le serveur.
    """
    client: Optional[pymongo.MongoClient] = None
    server_ok = False
    db_ok = False

    try:
        client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=2000)
        client.admin.command("ping")
        server_ok = True

        if MONGO_DB_NAME in client.list_database_names():
            db_ok = True
    except Exception:
        # Le d√©tail de l'exception est trait√© plus haut dans l'IHM.
        server_ok = False
        db_ok = False
    finally:
        if client is not None:
            client.close()

    return server_ok, db_ok


def log_progress(
    current: int,
    total: int,
    prefix: str,
    log_fn: Callable[[str, bool], None],
    step_percent: Optional[int] = None,
) -> None:
    """
    Trace la progression d'un traitement long dans les logs.

    Param√®tres
    ----------
    current : int
        Position actuelle dans le traitement.
    total : int
        Nombre total d'√©l√©ments √† traiter.
    prefix : str
        Libell√© du traitement (affich√© dans le message).
    log_fn : Callable[[str, bool], None]
        Fonction utilis√©e pour √©crire dans le journal (CLI ou Streamlit).
    step_percent : int | None
        Fr√©quence de rafra√Æchissement de l'affichage en pourcentage (10 -> tous les 10%).

    Retour
    ------
    None
    """
    if total == 0:
        return

    if step_percent:
        step = max(int(total * (step_percent / 100)), 1)
        # Mise √† jour limit√©e pour ne pas saturer les logs.
        if current % step != 0 and current != total:
            return

    pct = (current / total) * 100
    message = f"    >> {prefix} : {current:,} / {total:,} ({pct:.1f}%)"
    log_fn(message, replace_last=True)


# =====================================================================
# TRANSFORMATION DATAFRAME -> DOCUMENTS + SAUVEGARDE JSON / MONGO
# =====================================================================

def dataframe_to_dict_progressive(
    df: pd.DataFrame,
    label: str,
    log_fn: Callable[[str, bool], None],
    batch_size: int = 1000,
) -> List[Dict]:
    """
    Convertit un DataFrame en liste de dictionnaires avec journalisation progressive.

    Param√®tres
    ----------
    df : pandas.DataFrame
        Donn√©es en entr√©e.
    label : str
        Libell√© du traitement pour les messages de progression.
    log_fn : Callable[[str, bool], None]
        Fonction de log utilis√©e pour afficher la progression.
    batch_size : int
        Taille des paquets trait√©s √† chaque it√©ration.

    Retour
    ------
    list[dict]
        Liste compl√®te de documents pr√™ts √† √™tre ins√©r√©s en base.
    """
    total = len(df)
    documents: List[Dict] = []

    log_progress(0, total, label, log_fn)

    # Traitement par paquets pour limiter la consommation m√©moire et
    # √©viter d'inonder l'interface de logs.
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = df.iloc[start:end].to_dict("records")
        documents.extend(batch)
        log_progress(len(documents), total, label, log_fn)

    return documents


def sauvegarder_collection_json(
    nom_collection: str,
    data: List[Dict],
    log_fn: Callable[[str, bool], None],
) -> str:
    """
    Sauvegarde une collection m√©tier dans un fichier JSON unique.

    Param√®tres
    ----------
    nom_collection : str
        Nom fonctionnel de la collection (utilis√© pour le fichier).
    data : list[dict]
        Liste des documents √† s√©rialiser.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour suivre l'avancement.

    Retour
    ------
    str
        Nom du fichier JSON √©crit sur le disque.
    """
    file_name = f"Collection_{nom_collection}.json"
    full_path = os.path.join(DOSSIER_JSON, file_name)
    total = len(data)

    log_fn(
        f"   üíæ [JSON] Sauvegarde sur disque : {file_name} ({total:,} documents)...",
        replace_last=False,
    )

    try:
        with open(full_path, "w", encoding="utf-8") as json_file:
            # default=str permet de s√©rialiser les types non natifs (datetime, Timestamp...).
            json.dump(data, json_file, ensure_ascii=False, default=str)

        log_fn(
            f"   ‚úÖ [JSON] Fichier √©crit : {total:,} documents.",
            replace_last=False,
        )
        return file_name
    except Exception as exc:
        log_fn(f"   üí• [ERREUR] JSON : {exc}", replace_last=False)
        raise


def insert_with_progress(
    collection,
    docs: List[Dict],
    label: str,
    batch_size: int = 25000,
    log_fn: Callable[[str, bool], None] = print,
) -> None:
    """
    Ins√®re une liste de documents dans une collection MongoDB par gros paquets.

    Param√®tres
    ----------
    collection :
        Collection MongoDB cible.
    docs : list[dict]
        Documents √† ins√©rer.
    label : str
        Libell√© fonctionnel de la collection (pour les logs).
    batch_size : int
        Nombre de documents ins√©r√©s par batch.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour suivre la progression.

    Retour
    ------
    None
    """
    total = len(docs)
    if total == 0:
        return

    log_fn(
        f"   üì§ [MONGO] Injection de {total:,} documents dans '{label}'...",
        replace_last=False,
    )
    log_progress(0, total, "Insertion MongoDB", log_fn)

    inserted_count = 0

    # Insertion par paquets pour √©viter les demandes trop volumineuses
    # et laisser MongoDB optimiser l'ordre des insertions.
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = docs[start:end]
        collection.insert_many(batch, ordered=False)
        inserted_count += len(batch)
        log_progress(
            inserted_count,
            total,
            "Insertion MongoDB",
            log_fn,
            step_percent=10,
        )

    log_fn(
        f"   ‚ú® [OK] Collection '{label}' enti√®rement migr√©e.\n",
        replace_last=False,
    )


# =====================================================================
# CHARGEMENT DES TABLES SQLITE ET CONSTRUCTION DES DOCUMENTS METIER
# =====================================================================

def load_tables(
    conn: sqlite3.Connection,
    log_fn: Callable[[str, bool], None],
) -> Dict[str, pd.DataFrame]:
    """
    Charge les tables n√©cessaires depuis la base SQLite dans un dictionnaire de DataFrame.

    Param√®tres
    ----------
    conn : sqlite3.Connection
        Connexion SQLite d√©j√† ouverte.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour tracer les √©tapes de chargement.

    Retour
    ------
    dict[str, pandas.DataFrame]
        Dictionnaire mappant le nom de la table vers son contenu.
    """
    log_fn("üì• [SQL] Lecture des tables SQLite source...", replace_last=False)
    tables: Dict[str, pd.DataFrame] = {}
    table_names = [
        "Ligne",
        "Quartier",
        "Arret",
        "ArretQuartier",
        "Chauffeur",
        "Vehicule",
        "Horaire",
        "Capteur",
        "Mesure",
        "Trafic",
        "Incident",
    ]

    for table_name in table_names:
        tables[table_name] = pd.read_sql_query(
            f"SELECT * FROM {table_name}",
            conn,
        )

    log_fn(
        f"   üß± {len(tables)} tables charg√©es en m√©moire.",
        replace_last=False,
    )
    return tables

def build_lignes_docs(
    tables: Dict[str, pd.DataFrame],
    log_fn: Callable[[str, bool], None],
) -> List[Dict]:
    """
    Construit les documents 'lignes' avec une optimisation majeure (D√©normalisation)
    pour rendre les requ√™tes D et K instantan√©es.
    """
    df_l = tables["Ligne"]
    df_a = tables["Arret"]
    df_aq = tables["ArretQuartier"]
    df_q = tables["Quartier"]
    df_h = tables["Horaire"]
    df_v = tables["Vehicule"]
    df_c = tables["Chauffeur"]
    df_t = tables["Trafic"]
    df_i = tables["Incident"]
    df_cap = tables["Capteur"]
    df_mes = tables["Mesure"]

    # 1. Pr√©-calcul CO2 (inchang√©)
    co2_by_ligne: Dict[int, float] = {}
    if not df_cap.empty and not df_mes.empty:
        df_full = df_cap.merge(df_mes, on="id_capteur")
        df_co2 = df_full[df_full["type_capteur"] == "CO2"]
        df_co2_ligne = df_co2.merge(df_a[["id_arret", "id_ligne"]], on="id_arret")
        if not df_co2_ligne.empty:
            co2_by_ligne = df_co2_ligne.groupby("id_ligne")["valeur"].mean().to_dict()

    # 2. OPTIMISATION QUERY K : Cache Chauffeurs
    log_fn("‚ö° [OPTIM] Construction du cache Chauffeurs...", replace_last=False)
    chauffeurs_cache_by_ligne: Dict[int, List[Dict]] = {}
    if not df_v.empty and not df_c.empty:
        df_vc = df_v.merge(df_c, on="id_chauffeur", how="inner")
        if "id_ligne" in df_vc.columns:
            for id_ligne, group in df_vc.groupby("id_ligne"):
                if pd.isna(id_ligne): continue
                c_list = []
                for _, row in group.iterrows():
                    if pd.notnull(row.get("nom")):
                        c_list.append({
                            "id_chauffeur": int(row["id_chauffeur"]),
                            "nom_chauffeur": row["nom"]
                        })
                if c_list:
                    chauffeurs_cache_by_ligne[int(id_ligne)] = c_list

    # 3. OPTIMISATION QUERY D : Cache V√©hicules Uniques par Ligne
    # On √©vite l'unwind monstrueux des horaires
    log_fn("‚ö° [OPTIM] Construction du cache V√©hicules...", replace_last=False)
    vehicules_cache_by_ligne: Dict[int, List[Dict]] = {}
    if not df_v.empty:
        # On suppose que df_v a une colonne id_ligne (la ligne officielle du v√©hicule)
        for id_ligne, group in df_v.groupby("id_ligne"):
            if pd.isna(id_ligne): continue
            v_list = []
            for _, row in group.iterrows():
                if pd.notnull(row.get("immatriculation")):
                    v_list.append({
                        "id_vehicule": int(row["id_vehicule"]),
                        "immatriculation": row["immatriculation"]
                    })
            if v_list:
                vehicules_cache_by_ligne[int(id_ligne)] = v_list

    # 4. OPTIMISATION QUERY K & A : Stats Trafic pr√©-calcul√©es
    # On √©vite de scanner le tableau trafic en lecture
    log_fn("‚ö° [OPTIM] Pr√©-calcul des stats Trafic...", replace_last=False)
    stats_trafic_by_ligne: Dict[int, Dict] = {}
    trafic_by_ligne: Dict[int, List[Dict]] = {}
    
    # On pr√©pare aussi le d√©tail pour les autres requ√™tes, mais on calcule les stats ici
    incidents_by_trafic = {} 
    if not df_i.empty:
        for id_trafic, group in df_i.groupby("id_trafic"):
            incidents_by_trafic[id_trafic] = group[["id_incident", "description", "gravite"]].to_dict("records")

    if not df_t.empty:
        # Calcul vectoriel Pandas (beaucoup plus rapide que Mongo)
        stats_group = df_t.groupby("id_ligne")["retard_minutes"].agg(['sum', 'count', 'mean'])
        
        for id_ligne, row_stat in stats_group.iterrows():
            stats_trafic_by_ligne[int(id_ligne)] = {
                "total_retard": float(row_stat['sum']),
                "nb_trajets": int(row_stat['count']),
                "moyenne_precalc": float(row_stat['mean'])
            }

        # Construction standard du d√©tail trafic
        for _, row in df_t.iterrows():
            if pd.isna(row["id_ligne"]): continue
            tdoc = {"id_trafic": int(row["id_trafic"])}
            if pd.notnull(row.get("retard_minutes")): tdoc["retard_minutes"] = int(row["retard_minutes"])
            if row["id_trafic"] in incidents_by_trafic: tdoc["incidents"] = incidents_by_trafic[row["id_trafic"]]
            trafic_by_ligne.setdefault(int(row["id_ligne"]), []).append(tdoc)

    # 5. Pr√©parations standards (Arr√™ts, Quartiers, Horaires...)
    # (Code inchang√© pour la structure Arrets/Horaires, on le garde pour les requ√™tes B, L, etc.)
    quartiers_by_arret: Dict[int, List[Dict]] = {}
    if not df_aq.empty:
        tmp = df_aq.merge(df_q[["id_quartier", "nom"]].rename(columns={"nom": "nom_quartier"}), on="id_quartier", how="left")
        for id_arret, group in tmp.groupby("id_arret"):
            subset = group[["id_quartier", "nom_quartier"]].drop_duplicates("id_quartier")
            quartiers_by_arret[id_arret] = [
                {"id_quartier": int(row["id_quartier"]), "nom": row["nom_quartier"]}
                for _, row in subset.iterrows() if pd.notnull(row["id_quartier"])
            ]

    capteurs_ids_by_arret: Dict[int, List[int]] = {}
    if not df_cap.empty:
        for id_arret, group in df_cap.groupby("id_arret"):
            capteurs_ids_by_arret[id_arret] = [int(v) for v in group["id_capteur"].dropna().unique().tolist()]

    horaires_by_arret: Dict[int, List[Dict]] = {}
    if not df_h.empty:
        df_v_clean = df_v.rename(columns={"id_ligne": "id_ligne_officielle"})
        df_h_full = df_h.merge(df_v_clean, on="id_vehicule", how="left", suffixes=("", "_vehicule"))
        for col in ["heure_prevue"]:
             if col in df_h_full.columns: df_h_full[col] = pd.to_datetime(df_h_full[col], errors="coerce")

        total_rows = len(df_h_full)
        log_progress(0, total_rows, "Groupement Horaires/Arr√™ts", log_fn)

        for idx, row in enumerate(df_h_full.itertuples(index=False), start=1):
            if pd.isna(row.id_arret): continue
            
            # Construction all√©g√©e
            vehicule = {}
            if getattr(row, "id_vehicule", None) and not pd.isna(row.id_vehicule):
                vehicule["id_vehicule"] = int(row.id_vehicule)
                if hasattr(row, "type_vehicule"): vehicule["type_vehicule"] = row.type_vehicule
                if hasattr(row, "immatriculation"): vehicule["immatriculation"] = row.immatriculation
                if hasattr(row, "id_ligne_officielle") and pd.notnull(row.id_ligne_officielle):
                    vehicule["id_ligne_officielle"] = int(row.id_ligne_officielle)

            horaire = {}
            if hasattr(row, "heure_prevue") and pd.notnull(row.heure_prevue): horaire["heure_prevue"] = row.heure_prevue.to_pydatetime()
            if hasattr(row, "passagers_estimes") and pd.notnull(row.passagers_estimes): horaire["passagers_estimes"] = int(row.passagers_estimes)
            if vehicule: horaire["vehicule"] = vehicule
            
            horaires_by_arret.setdefault(int(row.id_arret), []).append(horaire)
            if idx % 10000 == 0: log_progress(idx, total_rows, "Groupement Horaires/Arr√™ts", log_fn)
        log_fn("", replace_last=False)

    arrets_by_ligne: Dict[int, List[Dict]] = {}
    for _, row in df_a.iterrows():
        if pd.isna(row["id_ligne"]) or pd.isna(row["id_arret"]): continue
        id_l, id_a = int(row["id_ligne"]), int(row["id_arret"])
        adoc = {"id_arret": id_a, "nom": row["nom"]}
        if id_a in quartiers_by_arret: adoc["quartiers"] = quartiers_by_arret[id_a]
        if id_a in horaires_by_arret: adoc["horaires"] = horaires_by_arret[id_a]
        if id_a in capteurs_ids_by_arret: adoc["capteurs_ids"] = capteurs_ids_by_arret[id_a]
        arrets_by_ligne.setdefault(id_l, []).append(adoc)

    # 6. Assemblage final
    docs = []
    total = len(df_l)
    log_progress(0, total, "Assemblage Lignes", log_fn)
    for idx, (_, row) in enumerate(df_l.iterrows(), start=1):
        if pd.isna(row["id_ligne"]): continue
        id_l = int(row["id_ligne"])
        doc = {
            "id_ligne": id_l, 
            "nom_ligne": row.get("nom_ligne"),
            "type": row.get("type")
        }
        
        # Injection CO2
        if id_l in co2_by_ligne: 
            doc["co2_moyen_ligne"] = co2_by_ligne[id_l]

        # Injection Cache Chauffeurs (Pour Query K)
        if id_l in chauffeurs_cache_by_ligne:
            doc["chauffeurs_cache"] = chauffeurs_cache_by_ligne[id_l]

        # Injection Cache V√©hicules (Pour Query D)
        if id_l in vehicules_cache_by_ligne:
            doc["vehicules_cache"] = vehicules_cache_by_ligne[id_l]
            
        # Injection Stats Trafic (Pour Query K et A)
        if id_l in stats_trafic_by_ligne:
            doc["stats_trafic"] = stats_trafic_by_ligne[id_l]

        if id_l in arrets_by_ligne: doc["arrets"] = arrets_by_ligne[id_l]
        if id_l in trafic_by_ligne: doc["trafic"] = trafic_by_ligne[id_l]
        if pd.notnull(row.get("frequentation_moyenne")): doc["frequentation_moyenne"] = float(row["frequentation_moyenne"])
        
        docs.append(doc)
        if idx % 100 == 0: log_progress(idx, total, "Assemblage Lignes", log_fn)

    return docs

def build_quartiers_docs(
    tables: Dict[str, pd.DataFrame],
    log_fn: Callable[[str, bool], None],
) -> List[Dict]:
    """
    Construit les documents de la collection 'quartiers' √† partir des tables SQLite.

    Param√®tres
    ----------
    tables : dict[str, pandas.DataFrame]
        Tables SQLite pr√©charg√©es.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour tracer la progression.

    Retour
    ------
    list[dict]
        Documents pr√™ts √† √™tre ins√©r√©s dans la collection 'quartiers'.
    """
    df_q = tables["Quartier"]
    df_aq = tables["ArretQuartier"]
    df_a = tables["Arret"]
    df_l = tables["Ligne"]

    arrets_by_quartier: Dict[int, List[Dict]] = {}
    if not df_aq.empty:
        log_fn(
            "Running : Groupement Arr√™ts par quartier...",
            replace_last=False,
        )
        df_aq_full = df_aq.merge(
            df_a[["id_arret", "nom", "id_ligne"]].rename(
                columns={"nom": "nom_arret"},
            ),
            on="id_arret",
            how="left",
        )
        df_aq_full = df_aq_full.merge(
            df_l[["id_ligne", "nom_ligne"]],
            on="id_ligne",
            how="left",
        )

        groups = df_aq_full.groupby("id_quartier")
        nb_groups = len(groups)
        log_progress(0, nb_groups, "Groupement Arr√™ts/Quartiers", log_fn)

        for idx, (id_quartier, group) in enumerate(groups, start=1):
            subset = group[
                [
                    "id_arret",
                    "nom_arret",
                    "id_ligne",
                    "nom_ligne",
                ]
            ].drop_duplicates("id_arret")

            stops: List[Dict] = []
            for _, row in subset.iterrows():
                if pd.isna(row["id_arret"]):
                    continue
                stop_doc: Dict[str, object] = {
                    "id_arret": int(row["id_arret"]),
                    "nom": row["nom_arret"],
                }
                if pd.notnull(row.get("id_ligne")):
                    stop_doc["id_ligne"] = int(row["id_ligne"])
                if pd.notnull(row.get("nom_ligne")):
                    stop_doc["nom_ligne"] = row["nom_ligne"]
                stops.append(stop_doc)

            arrets_by_quartier[id_quartier] = stops
            log_progress(
                idx,
                nb_groups,
                "Groupement Arr√™ts/Quartiers",
                log_fn,
                step_percent=10,
            )
        log_fn("", replace_last=False)

    docs: List[Dict] = []
    total_quartiers = len(df_q)
    label_final = "Construction documents quartiers"
    log_progress(0, total_quartiers, label_final, log_fn)

    for idx, (_, row) in enumerate(df_q.iterrows(), start=1):
        if pd.isna(row["id_quartier"]):
            continue

        id_quartier = int(row["id_quartier"])
        doc: Dict[str, object] = {
            "id_quartier": id_quartier,
            "nom": row.get("nom"),
        }

        geom = parse_geojson_geometry(row.get("geojson"))
        if geom is not None:
            doc["geom"] = geom

        stops = arrets_by_quartier.get(id_quartier)
        if stops:
            doc["arrets"] = stops

        docs.append(doc)
        if (idx % 100 == 0) or (idx == total_quartiers):
            log_progress(idx, total_quartiers, label_final, log_fn)

    log_fn("", replace_last=False)
    return docs


def build_capteurs_docs(
    tables: Dict[str, pd.DataFrame],
    log_fn: Callable[[str, bool], None],
) -> List[Dict]:
    """
    Construit les documents de la collection 'capteurs' √† partir des tables SQLite.

    Param√®tres
    ----------
    tables : dict[str, pandas.DataFrame]
        Tables SQLite pr√©charg√©es.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour suivre le traitement.

    Retour
    ------
    list[dict]
        Documents pr√™ts √† √™tre ins√©r√©s dans la collection 'capteurs'.
    """
    df_capteur = tables["Capteur"]
    df_mesure = tables["Mesure"]
    df_arret = tables["Arret"]
    df_ligne = tables["Ligne"]

    arret_info_by_id: Dict[int, Dict] = {}
    if not df_arret.empty:
        df_a_l = df_arret.merge(
            df_ligne[["id_ligne", "nom_ligne"]],
            on="id_ligne",
            how="left",
        )
        for _, row in df_a_l.iterrows():
            if pd.isna(row["id_arret"]):
                continue

            arret_id = int(row["id_arret"])
            info: Dict[str, object] = {
                "id_arret": arret_id,
                "nom": row.get("nom"),
            }
            if "id_ligne" in row and pd.notnull(row["id_ligne"]):
                info["id_ligne"] = int(row["id_ligne"])
            if "nom_ligne" in row and pd.notnull(row["nom_ligne"]):
                info["nom_ligne"] = row["nom_ligne"]
            arret_info_by_id[arret_id] = info

    type_capteur_by_id: Dict[int, Optional[str]] = {}
    for _, row in df_capteur.iterrows():
        if pd.isna(row["id_capteur"]):
            continue

        capteur_id = int(row["id_capteur"])
        type_capteur_by_id[capteur_id] = row.get("type_capteur")

    mesures_by_capteur: Dict[int, List[Dict]] = {}
    if not df_mesure.empty:
        log_fn(
            "Running : Groupement Mesures par capteur...",
            replace_last=False,
        )
        groups = df_mesure.groupby("id_capteur")
        nb_groups = len(groups)
        log_progress(0, nb_groups, "Groupement Mesures/Capteurs", log_fn)
        has_unite_col = "unite" in df_mesure.columns

        for idx, (capteur_id, group) in enumerate(groups, start=1):
            meas_list: List[Dict] = []
            for _, row in group.iterrows():
                meas_doc: Dict[str, object] = {}
                if "id_mesure" in row and pd.notnull(row["id_mesure"]):
                    meas_doc["id_mesure"] = int(row["id_mesure"])
                if "horodatage" in row and pd.notnull(row["horodatage"]):
                    dt_measure = pd.to_datetime(
                        row["horodatage"],
                        errors="coerce",
                    )
                    if not pd.isna(dt_measure):
                        meas_doc["horodatage"] = dt_measure.to_pydatetime()
                if "valeur" in row and pd.notnull(row["valeur"]):
                    try:
                        meas_doc["valeur"] = float(row["valeur"])
                    except Exception:
                        pass

                if has_unite_col and pd.notnull(row.get("unite")):
                    unite_val = row["unite"]
                else:
                    type_cap = type_capteur_by_id.get(int(capteur_id))
                    unite_val = infer_unite_from_type(type_cap)

                if unite_val is not None:
                    meas_doc["unite"] = unite_val

                meas_list.append(meas_doc)

            mesures_by_capteur[int(capteur_id)] = meas_list
            log_progress(
                idx,
                nb_groups,
                "Groupement Mesures/Capteurs",
                log_fn,
                step_percent=10,
            )
        log_fn("", replace_last=False)

    docs: List[Dict] = []
    total_capteurs = len(df_capteur)
    label_final = "Construction documents capteurs"
    log_progress(0, total_capteurs, label_final, log_fn)

    for idx, (_, row) in enumerate(df_capteur.iterrows(), start=1):
        if pd.isna(row["id_capteur"]):
            continue

        capteur_id = int(row["id_capteur"])
        doc: Dict[str, object] = {"id_capteur": capteur_id}

        if "type_capteur" in row and pd.notnull(row["type_capteur"]):
            doc["type_capteur"] = row["type_capteur"]

        latitude = row.get("latitude")
        longitude = row.get("longitude")
        if pd.notnull(latitude) and pd.notnull(longitude):
            doc["position"] = {
                "type": "Point",
                "coordinates": [float(longitude), float(latitude)],
            }

        if "id_arret" in row and pd.notnull(row["id_arret"]):
            arret_id = int(row["id_arret"])
            info = arret_info_by_id.get(arret_id)
            if info:
                doc["arret"] = info

        doc["mesures"] = mesures_by_capteur.get(capteur_id, [])

        docs.append(doc)
        if (idx % 500 == 0) or (idx == total_capteurs):
            log_progress(idx, total_capteurs, label_final, log_fn)

    log_fn("", replace_last=False)
    return docs


# =====================================================================
# ORCHESTRATION MIGRATION SQLITE -> MONGODB
# =====================================================================

def creer_index_mongodb(
    db: pymongo.database.Database,
    log_fn: Callable[[str, bool], None],
) -> None:
    """
    Cr√©e les index utiles pour acc√©l√©rer les principales requ√™tes MongoDB.

    Param√®tres
    ----------
    db : pymongo.database.Database
        Base MongoDB 'Paris2055'.
    log_fn : Callable[[str, bool], None]
        Fonction de log pour tracer la cr√©ation des index.

    Retour
    ------
    None
    """
    log_fn("[Index] Cr√©ation des index MongoDB...", replace_last=False)
    try:
        db.lignes.create_index("id_ligne")
        db.lignes.create_index("nom_ligne")
        db.lignes.create_index("type")

        db.capteurs.create_index("id_capteur")
        db.capteurs.create_index("type_capteur")
        db.capteurs.create_index("arret.id_ligne")
        db.capteurs.create_index("arret.id_arret")

        db.quartiers.create_index([("geom", "2dsphere")])
        db.capteurs.create_index([("position", "2dsphere")])

        #            le nouveau mod√®le stockant les arr√™ts imbriqu√©s dans 'lignes'
        #              et 'quartiers'. Cela √©vite de cr√©er une collection vide.
        log_fn("[Index] Index cr√©√©s avec succ√®s.", replace_last=False)
    except Exception as exc:
        log_fn(
            f"[Index] Erreur lors de la cr√©ation des index : {exc}",
            replace_last=False,
        )


def migrer_sqlite_vers_mongo(
    log_fn_raw: Callable[[str, bool], None],
) -> None:
    """
    Lance la migration compl√®te de SQLite vers MongoDB pour le mod√®le document.

    √âtapes
    ------
    - Nettoyage d'anciennes collections cibles.
    - Lecture des tables SQLite.
    - Construction des documents m√©tier pour 'lignes', 'quartiers', 'capteurs'.
    - Sauvegarde en JSON interm√©diaire.
    - Insertion des documents dans MongoDB.
    - Cr√©ation des index.

    Param√®tres
    ----------
    log_fn_raw : Callable[[str, bool], None]
        Fonction de log fournie par l'IHM (Streamlit).

    Retour
    ------
    None
    """

    def secure_log(message: str, replace_last: bool = False) -> None:
        """
        Adapte la fonction de log fournie pour supporter ou non l'argument
        'replace_last'.

        Cette fonction garantit que la migration reste fonctionnelle m√™me
        si l'appelant ne g√®re pas ce param√®tre optionnel.
        """
        try:
            log_fn_raw(message, replace_last=replace_last)
        except TypeError:
            print(message)

    def process_step(
        label: str,
        build_func: Callable[[Dict[str, pd.DataFrame], Callable[[str, bool], None]], List[Dict]],
        tables: Dict[str, pd.DataFrame],
    ) -> None:
        """
        Ex√©cute une √©tape de migration compl√®te pour une collection.

        Param√®tres
        ----------
        label : str
            Nom de la collection MongoDB cible.
        build_func : Callable
            Fonction de construction des documents m√©tier.
        tables : dict[str, pandas.DataFrame]
            Ensemble des tables SQLite pr√©charg√©es.
        """
        secure_log(
            f"\nüîπ --- TRAITEMENT COLLECTION : {label.upper()} ---",
            replace_last=False,
        )

        secure_log(
            "   ‚öôÔ∏è  Construction du mod√®le m√©tier...",
            replace_last=False,
        )
        documents = build_func(tables, secure_log)
        secure_log(
            f"   üëå  Construction termin√©e : {len(documents):,} documents pr√™ts.",
            replace_last=False,
        )

        sauvegarder_collection_json(label, documents, secure_log)

        insert_with_progress(
            db[label],
            documents,
            label,
            log_fn=secure_log,
        )

        # Lib√©ration m√©moire volontaire pour les gros volumes.
        del documents

    client = pymongo.MongoClient(MONGO_URI)
    db = client[MONGO_DB_NAME]

    secure_log("üöÄ D√âBUT DE LA MIGRATION (ETL)", replace_last=False)
    secure_log(
        "   Mode : SQLite -> Mod√®le Document -> MongoDB",
        replace_last=False,
    )

    try:
        secure_log(
            "\nüßπ [INIT] Nettoyage de la base cible...",
            replace_last=False,
        )
        old_collections = [
            "Lignes",
            "Quartiers",
            "Vehicules",
            "Capteurs",
            "Trafics",
            "Arrets",
            "lignes",
            "quartiers",
            "capteurs",
        ]
        dropped_count = 0
        try:
            existing_collections = set(db.list_collection_names())
        except PyMongoError:
            existing_collections = set()

        for collection_name in old_collections:
            if collection_name in existing_collections:
                try:
                    db[collection_name].drop()
                    dropped_count += 1
                except PyMongoError:
                    # L'√©chec de suppression d'une collection ne doit pas bloquer
                    # l'ensemble du processus de migration.
                    continue

        secure_log(
            f"   üóëÔ∏è  {dropped_count} anciennes collections supprim√©es.",
            replace_last=False,
        )

        if not os.path.exists(DB_FILE):
            raise FileNotFoundError(f"DB introuvable: {DB_FILE}")

        sqlite_conn = sqlite3.connect(DB_FILE)
        try:
            tables = load_tables(sqlite_conn, secure_log)
        finally:
            sqlite_conn.close()

        process_step("lignes", build_lignes_docs, tables)
        process_step("quartiers", build_quartiers_docs, tables)
        process_step("capteurs", build_capteurs_docs, tables)

        secure_log(
            "\nüîé [INDEX] Optimisation de la base...",
            replace_last=False,
        )
        creer_index_mongodb(db, secure_log)

        secure_log(
            "\nüéâ --- MIGRATION TERMIN√âE AVEC SUCC√àS ---",
            replace_last=False,
        )

    except Exception as exc:
        secure_log(f"\nüí• [CRITICAL ERROR]: {exc}", replace_last=False)
    finally:
        client.close()


# =====================================================================
# REQUETES SQL (PARTIE 1) ET CACHE CSV
# =====================================================================

REQUETES_OBJECTIFS: Dict[str, str] = {
    "A": (
        "Calculer la moyenne des retards (en minutes) pour chaque ligne de "
        "transport, tri√©e par ordre d√©croissant."
    ),
    "B": (
        "Estimer le nombre moyen de passagers transport√©s par jour pour "
        "chaque ligne."
    ),
    "C": (
        "Calculer le taux d'incidents (en pourcentage) pour chaque ligne, "
        "bas√© sur le nombre de trajets ayant signal√© un incident."
    ),
    "D": (
        "Identifier la moyenne d'√©mission de CO2 (capt√©e aux arr√™ts) "
        "associ√©e aux v√©hicules, tri√©e par ordre d√©croissant."
    ),
    "E": (
        "Trouver les 5 quartiers ayant la moyenne de niveau de bruit (en dB) "
        "la plus √©lev√©e, bas√©e sur les capteurs de bruit aux arr√™ts."
    ),
    "F": (
        "Identifier les lignes ayant eu des retards de plus de 10 minutes, "
        "mais qui n'ont pas eu d'incidents signal√©s (EXCEPT)."
    ),
    "G": (
        "Calculer le taux de ponctualit√© global (en pourcentage) de tous les "
        "trajets enregistr√©s (retard = 0)."
    ),
    "H": (
        "Compter le nombre d'arr√™ts desservis par chaque quartier, tri√© par "
        "ordre d√©croissant."
    ),
    "I": (
        "Corr√©ler la moyenne des retards et la moyenne des √©missions de CO2 "
        "pour chaque ligne (jointures externes)."
    ),
    "J": (
        "Calculer la moyenne de temp√©rature (capt√©e aux arr√™ts) pour chaque "
        "ligne de transport."
    ),
    "K": (
        "Calculer la moyenne des retards (en minutes) associ√©e aux trajets "
        "effectu√©s par chaque chauffeur."
    ),
    "L": (
        "Pour les lignes de Bus uniquement, calculer le pourcentage de "
        "v√©hicules √©lectriques dans leur flotte."
    ),
    "M": (
        "Classifier les capteurs de CO2 par niveau de pollution ('√âlev√©', "
        "'Moyen', 'Faible') en fonction de la moyenne de leurs mesures."
    ),
    "N": (
        "Classifier les lignes par cat√©gorie de fr√©quentation ('Haute', "
        "'Moyenne', 'Basse') bas√©e sur leur fr√©quentation moyenne."
    ),
}


def executer_toutes_les_requetes() -> Dict[str, pd.DataFrame]:
    """
    Ex√©cute l'ensemble des requ√™tes SQL d√©finies A -> N sur la base SQLite.

    Retour
    ------
    dict[str, pandas.DataFrame]
        Dictionnaire associant le code de requ√™te √† son DataFrame r√©sultat.
    """
    if not os.path.exists(DB_FILE):
        raise FileNotFoundError(DB_FILE)

    resultats: Dict[str, pd.DataFrame] = {}

    sql_queries: Dict[str, str] = {
        "A": (
            "SELECT L.nom_ligne, "
            "AVG(T.retard_minutes) AS moyenne_retard_minutes "
            "FROM Trafic AS T "
            "JOIN Ligne AS L ON T.id_ligne = L.id_ligne "
            "GROUP BY L.nom_ligne "
            "ORDER BY moyenne_retard_minutes DESC;"
        ),
        "B": (
            "SELECT "
            "    L.id_ligne, "
            "    AVG(T.total_passagers_jour) AS moyenne_passagers_jour "
            "FROM ( "
            "    SELECT "
            "        A.id_ligne, "
            "        DATE(H.heure_prevue) AS jour, "
            "        SUM(H.passagers_estimes) AS total_passagers_jour "
            "    FROM Horaire AS H "
            "    JOIN Arret AS A ON H.id_arret = A.id_arret "
            "    GROUP BY "
            "        A.id_ligne, "
            "        jour "
            ") AS T "
            "JOIN Ligne AS L ON T.id_ligne = L.id_ligne "
            "GROUP BY L.id_ligne "
            "ORDER BY moyenne_passagers_jour DESC;"
        ),
        "C": (
            "SELECT L.nom_ligne, "
            "COUNT(DISTINCT I.id_trafic) AS nb_trafic_avec_incident, "
            "COUNT(DISTINCT T.id_trafic) AS nb_total_trafic, "
            "CASE "
            "WHEN COUNT(DISTINCT T.id_trafic) = 0 THEN 0 "
            "ELSE "
            "(CAST(COUNT(DISTINCT I.id_trafic) AS REAL) "
            "/ COUNT(DISTINCT T.id_trafic)) * 100 "
            "END AS taux_incident_pourcent "
            "FROM Ligne AS L "
            "LEFT JOIN Trafic AS T ON L.id_ligne = T.id_ligne "
            "LEFT JOIN Incident AS I ON T.id_trafic = I.id_trafic "
            "GROUP BY L.nom_ligne "
            "ORDER BY taux_incident_pourcent DESC;"
        ),
        "D": (
            "SELECT V.id_vehicule, V.immatriculation, "
            "AVG(M.valeur) AS moyenne_co2 "
            "FROM Vehicule AS V "
            "JOIN Ligne AS L ON V.id_ligne = L.id_ligne "
            "JOIN Arret AS A ON L.id_ligne = A.id_ligne "
            "JOIN Capteur AS C ON A.id_arret = C.id_arret "
            "JOIN Mesure AS M ON C.id_capteur = M.id_capteur "
            "WHERE C.type_capteur = 'CO2' "
            "GROUP BY V.id_vehicule, V.immatriculation "
            "ORDER BY moyenne_co2 DESC;"
        ),
        "E": (
            "SELECT Q.nom, "
            "AVG(M.valeur) AS moyenne_bruit_db "
            "FROM Quartier AS Q "
            "JOIN ArretQuartier AS AQ ON Q.id_quartier = AQ.id_quartier "
            "JOIN Arret AS A ON AQ.id_arret = A.id_arret "
            "JOIN Capteur AS C ON A.id_arret = C.id_arret "
            "JOIN Mesure AS M ON C.id_capteur = M.id_capteur "
            "WHERE C.type_capteur = 'Bruit' "
            "GROUP BY Q.nom "
            "ORDER BY moyenne_bruit_db DESC "
            "LIMIT 5;"
        ),
        "F": (
            "SELECT DISTINCT L.nom_ligne "
            "FROM Ligne AS L "
            "JOIN Trafic AS T ON L.id_ligne = T.id_ligne "
            "WHERE T.retard_minutes > 10 "
            "EXCEPT "
            "SELECT DISTINCT L.nom_ligne "
            "FROM Ligne AS L "
            "JOIN Trafic AS T ON L.id_ligne = T.id_ligne "
            "JOIN Incident AS I ON T.id_trafic = I.id_trafic;"
        ),
        "G": (
            "SELECT "
            "COUNT(*) AS total_trajets, "
            "SUM(CASE WHEN retard_minutes = 0 THEN 1 ELSE 0 END) "
            "AS trajets_sans_retard, "
            "(CAST(SUM(CASE WHEN retard_minutes = 0 THEN 1 ELSE 0 END) "
            "AS REAL) / COUNT(*)) * 100 "
            "AS taux_ponctualite_global_pourcent "
            "FROM Trafic;"
        ),
        "H": (
            "SELECT Q.nom, "
            "COUNT(AQ.id_arret) AS nombre_arrets "
            "FROM Quartier AS Q "
            "LEFT JOIN ArretQuartier AS AQ "
            "ON Q.id_quartier = AQ.id_quartier "
            "GROUP BY Q.nom "
            "ORDER BY nombre_arrets DESC;"
        ),
        "I": (
            "WITH AvgRetard AS ( "
            "  SELECT id_ligne, "
            "         AVG(retard_minutes) AS moyenne_retard "
            "  FROM Trafic "
            "  GROUP BY id_ligne "
            "), "
            "AvgCO2 AS ( "
            "  SELECT A.id_ligne, "
            "         AVG(M.valeur) AS moyenne_co2 "
            "  FROM Mesure AS M "
            "  JOIN Capteur AS C ON M.id_capteur = C.id_capteur "
            "  JOIN Arret AS A ON C.id_arret = A.id_arret "
            "  WHERE C.type_capteur = 'CO2' "
            "  GROUP BY A.id_ligne "
            ") "
            "SELECT L.nom_ligne, "
            "COALESCE(R.moyenne_retard, 0) AS moyenne_retard, "
            "COALESCE(C.moyenne_co2, 0) AS moyenne_co2 "
            "FROM Ligne AS L "
            "LEFT JOIN AvgRetard AS R ON L.id_ligne = R.id_ligne "
            "LEFT JOIN AvgCO2 AS C ON L.id_ligne = C.id_ligne "
            "ORDER BY L.nom_ligne;"
        ),
        "J": (
            "SELECT L.nom_ligne, "
            "AVG(M.valeur) AS moyenne_temperature "
            "FROM Ligne AS L "
            "JOIN Arret AS A ON L.id_ligne = A.id_ligne "
            "JOIN Capteur AS C ON A.id_arret = C.id_arret "
            "JOIN Mesure AS M ON C.id_capteur = M.id_capteur "
            "WHERE C.type_capteur = 'Temperature' "
            "GROUP BY L.nom_ligne "
            "ORDER BY moyenne_temperature DESC;"
        ),
        "K": (
            "SELECT C.nom, "
            "AVG(T.retard_minutes) AS moyenne_retard_minutes "
            "FROM Chauffeur AS C "
            "JOIN Vehicule AS V ON C.id_chauffeur = V.id_chauffeur "
            "JOIN Trafic AS T ON V.id_ligne = T.id_ligne "
            "GROUP BY C.nom "
            "ORDER BY moyenne_retard_minutes DESC;"
        ),
        "L": (
            "SELECT L.nom_ligne, "
            "COUNT(V.id_vehicule) AS total_vehicules, "
            "SUM(CASE WHEN V.type_vehicule = 'Electrique' THEN 1 ELSE 0 END) "
            "AS nb_electriques, "
            "(CAST(SUM(CASE WHEN V.type_vehicule = 'Electrique' THEN 1 "
            "ELSE 0 END) AS REAL) / COUNT(V.id_vehicule)) * 100 "
            "AS pourcentage_electrique "
            "FROM Ligne AS L "
            "JOIN Vehicule AS V ON L.id_ligne = V.id_ligne "
            "WHERE L.type = 'Bus' "
            "GROUP BY L.nom_ligne "
            "ORDER BY pourcentage_electrique DESC;"
        ),
        "M": (
            "SELECT C.id_capteur, "
            "C.latitude, "
            "C.longitude, "
            "AVG(M.valeur) AS moyenne_co2, "
            "CASE "
            "  WHEN AVG(M.valeur) > 800 THEN '√âlev√©' "
            "  WHEN AVG(M.valeur) > 450 THEN 'Moyen' "
            "  ELSE 'Faible' "
            "END AS niveau_pollution "
            "FROM Capteur AS C "
            "JOIN Mesure AS M ON C.id_capteur = M.id_capteur "
            "WHERE C.type_capteur = 'CO2' "
            "GROUP BY C.id_capteur, C.latitude, C.longitude "
            "ORDER BY moyenne_co2 DESC;"
        ),
        "N": (
            "SELECT nom_ligne, "
            "type, "
            "frequentation_moyenne, "
            "CASE "
            "  WHEN frequentation_moyenne > 2000 THEN 'Haute Fr√©quentation' "
            "  WHEN frequentation_moyenne > 1000 THEN 'Moyenne Fr√©quentation' "
            "  ELSE 'Basse Fr√©quentation' "
            "END AS categorie_frequentation "
            "FROM Ligne "
            "ORDER BY frequentation_moyenne DESC;"
        ),
    }

    with sqlite3.connect(DB_FILE) as conn:
        for code, query in sql_queries.items():
            try:
                df_result = pd.read_sql_query(query, conn)
                enregistrer_resultats_csv(
                    DOSSIER_CSV,
                    f"resultat_req_{code.lower()}.csv",
                    df_result,
                )
                resultats[code] = df_result
            except Exception as exc:
                resultats[code] = pd.DataFrame([{"erreur": str(exc)}])

    return resultats


def charger_cache_csv() -> tuple[Dict[str, pd.DataFrame], bool]:
    """
    Recharge, si disponibles, les r√©sultats SQL pr√©c√©demment export√©s en CSV.

    Retour
    ------
    (dict[str, pandas.DataFrame], bool)
        - Dictionnaire des r√©sultats trouv√©s (par code de requ√™te).
        - Bool√©en indiquant si au moins un CSV a √©t√© retrouv√©.
    """
    print("Chargement cache CSV...", end="\n")
    resultats: Dict[str, pd.DataFrame] = {}
    found_any = False

    for code in REQUETES_OBJECTIFS.keys():
        path = os.path.join(DOSSIER_CSV, f"resultat_req_{code.lower()}.csv")
        if os.path.exists(path):
            try:
                resultats[code] = pd.read_csv(path)
                found_any = True
            except Exception:
                resultats[code] = pd.DataFrame()

    if found_any:
        print("CSV charg√©s ‚úÖ")
    else:
        print(" Non trouve")
    print("\n")
    return resultats, found_any


def charger_cache_csv_mongo() -> tuple[Dict[str, pd.DataFrame], bool]:
    """
    Recharge, si disponibles, les r√©sultats MongoDB pr√©c√©demment export√©s en CSV.

    Retour
    ------
    (dict[str, pandas.DataFrame], bool)
        - Dictionnaire des r√©sultats trouv√©s (par code de requ√™te).
        - Bool√©en indiquant si au moins un CSV a √©t√© retrouv√©.
    """
    print("Chargement cache CSV MongoDB...", end="\n")
    resultats: Dict[str, pd.DataFrame] = {}
    found_any = False

    for code in REQUETES_OBJECTIFS.keys():
        path = os.path.join(
            DOSSIER_MONGO_CSV,
            f"resultat_req_{code.lower()}.csv",
        )
        if os.path.exists(path):
            try:
                resultats[code] = pd.read_csv(path)
                found_any = True
            except Exception:
                resultats[code] = pd.DataFrame()

    if found_any:
        print("CSV MongoDB charg√©s ‚úÖ")
    else:
        print(" Non trouve")
    print("\n")
    return resultats, found_any

# =====================================================================
# ETAT ET COMPOSANTS STREAMLIT
# =====================================================================

#            entre logique m√©tier (ETL / requ√™tes) et pr√©sentation.


# Cr√©ation des dossiers n√©cessaires au d√©marrage de l'application.
os.makedirs(DOSSIER_DATA, exist_ok=True)
os.makedirs(os.path.dirname(DB_FILE), exist_ok=True)
os.makedirs(DOSSIER_CSV, exist_ok=True)
os.makedirs(DOSSIER_JSON, exist_ok=True)
os.makedirs(DOSSIER_MONGO_CSV, exist_ok=True)

def init_session_state() -> None:
    """
    Initialise les variables de session Streamlit.
    """
    if st.session_state.get("initialized", False):
        return

    # --- AJOUT GESTION API KEY ---
    # On charge la cl√© du .env par d√©faut dans la session
    if "groq_api_key" not in st.session_state:
        st.session_state["groq_api_key"] = os.getenv("GROQ_API_KEY", "")
    # -----------------------------

    st.session_state["requetes_objectifs"] = REQUETES_OBJECTIFS

    resultats_sql, sql_cache_found = charger_cache_csv()
    st.session_state["resultats_sql"] = resultats_sql
    st.session_state["queries_sql_executed"] = sql_cache_found

    resultats_mongo, mongo_cache_found = charger_cache_csv_mongo()
    st.session_state["resultats_mongo"] = resultats_mongo
    st.session_state["queries_mongo_executed"] = mongo_cache_found

    st.session_state["migration_logs"] = []
    st.session_state["migration_done_msg"] = ""
    st.session_state["migration_running"] = False
    
    st.session_state["ai_json_response"] = None 
    st.session_state["ai_question_text_value"] = ""

    st.session_state["initialized"] = True

# =====================================================================
# Partie 1 : REQU√äTES SQLITE
# =====================================================================

def render_partie_1_sqlite(tab) -> None:
    """
    Affiche la Partie 1 : ex√©cution et visualisation des requ√™tes SQLite.

    Param√®tres
    ----------
    tab :
        Conteneur Streamlit (onglet) dans lequel les √©l√©ments sont rendus.
    """
    with tab:
        st.subheader("Partie 1 : Requ√™tes SQLite")

        status_text = (
            "Donn√©es charg√©es."
            if st.session_state["queries_sql_executed"]
            else "Donn√©es non charg√©es."
        )
        st.write(status_text)

        if st.button("Executer Requetes", key="btn_sql_run"):
            with st.spinner("Ex√©cution des requ√™tes SQLite..."):
                res = executer_toutes_les_requetes()
            st.session_state["resultats_sql"] = res
            st.session_state["queries_sql_executed"] = True
            st.success("‚úÖ Requ√™tes termin√©es.")

        st.markdown("---")

        if not st.session_state["queries_sql_executed"]:
            st.info(
                "Les r√©sultats ne sont pas encore disponibles. "
                "Cliquez sur ¬´ Executer Requetes ¬ª pour lancer les requ√™tes.",
            )
            return

        st.markdown("### R√©sultats d√©taill√©s des requ√™tes SQLite")

        for code, objectif in st.session_state["requetes_objectifs"].items():
            df = st.session_state["resultats_sql"].get(code)
            with st.expander(
                f"Requ√™te {code} ‚Äì {objectif}",
                expanded=False,
            ):
                st.markdown(f"**Objectif :** {objectif}")
                if df is None:
                    st.warning("Aucun r√©sultat pour cette requ√™te.")
                elif df.empty:
                    st.info(
                        "La requ√™te n'a retourn√© aucun enregistrement.",
                    )
                else:
                    st.dataframe(
                        df.style.set_properties(
                            **{"text-align": "left"},
                        ),
                        width="content",
                    )

# =====================================================================
# Partie 2 : MIGRATION SQLITE -> MONGODB
# =====================================================================

def streamlit_migration_log(
    message: str,
    replace_last: bool = False,
) -> None:
    """
    Fonction de log sp√©cifique √† la migration, synchronisant le journal
    en temps r√©el avec l'interface Streamlit.

    Param√®tres
    ----------
    message : str
        Message √† afficher dans le journal de migration.
    replace_last : bool
        Si True, remplace la derni√®re entr√©e (progression), sinon ajoute
        une nouvelle ligne.
    """
    global MIGRATION_LOG_PLACEHOLDER

    logs: List[str] = st.session_state.get("migration_logs", [])
    if replace_last and logs:
        logs[-1] = message
    else:
        logs.append(message)
    st.session_state["migration_logs"] = logs

    if MIGRATION_LOG_PLACEHOLDER is not None:
        display_lines = logs[-MAX_LOG_LINES:]
        text_content = "\n".join(display_lines)
        MIGRATION_LOG_PLACEHOLDER.code(text_content, language="text")
        

def render_partie_2_migration(tab) -> None:
    """
    Affiche la Partie 2 : pilotage de la migration SQLite -> MongoDB.

    Param√®tres
    ----------
    tab :
        Conteneur Streamlit (onglet) dans lequel les √©l√©ments sont rendus.
    """
    global MIGRATION_LOG_PLACEHOLDER

    with tab:
        st.subheader("Partie 2 : Migration vers MongoDB")
        st.caption(
            "Cliquez pour migrer les donn√©es de SQLite vers MongoDB.",
        )

        def start_migration_callback() -> None:
            """
            Callback appel√© au clic sur le bouton de migration pour
            marquer le d√©marrage du traitement.
            """
            st.session_state["migration_running"] = True
            st.session_state["migration_logs"] = []

        col_btn, col_status = st.columns([1, 3], gap="small")

        with col_btn:
            st.button(
                "Lancer Migration",
                key="btn_migration",
                width="content",
                on_click=start_migration_callback,
                disabled=st.session_state["migration_running"],
            )

        MIGRATION_LOG_PLACEHOLDER = st.empty()

        if st.session_state["migration_running"]:
            MIGRATION_LOG_PLACEHOLDER.code(
                "Initialisation du processus...",
                language="text",
            )

            with col_status:
                with st.spinner(
                    "Migration en cours... Le bouton est d√©sactiv√©.",
                ):
                    migrer_sqlite_vers_mongo(log_fn_raw=streamlit_migration_log)

            st.session_state["migration_done_msg"] = (
                "Migration termin√©e avec succ√®s !"
            )
            st.session_state["migration_running"] = False

            st.toast("Migration termin√©e !", icon="üéâ")
            time.sleep(1)
            st.rerun()

        if st.session_state.get("migration_done_msg"):
            with col_status:
                st.success(st.session_state["migration_done_msg"])

        logs: List[str] = st.session_state.get("migration_logs", [])
        if logs:
            log_text = "\n".join(logs[-MAX_LOG_LINES:])
        else:
            log_text = "Pr√™t √† lancer la migration."

        MIGRATION_LOG_PLACEHOLDER.text_area(
            label="Journal de migration",
            value=log_text,
            height=400,
        )

# =====================================================================
# REQUETES MONGODB (PARTIE 3)
# =====================================================================
# =====================================================================
# REQUETES MONGODB (PARTIE 3)
# =====================================================================

def query_A_mongo(db) -> pd.DataFrame:
    """
    Requ√™te A (MongoDB).
    SQL: nom_ligne, moyenne_retard_minutes
    """
    pipeline = [
        {"$unwind": "$trafic"},
        {"$match": {"trafic.retard_minutes": {"$ne": None}}},
        {
            "$group": {
                "_id": "$nom_ligne",
                "moyenne_retard_minutes": {"$avg": "$trafic.retard_minutes"},
            },
        },
        {"$sort": {"moyenne_retard_minutes": -1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": "$_id",
                "moyenne_retard_minutes": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    # Force l'ordre des colonnes
    return df[["nom_ligne", "moyenne_retard_minutes"]] if not df.empty else df

def query_B_mongo(db):
    """
    Requ√™te B en MongoDB :
    Estimer le nombre moyen de passagers transport√©s par jour pour chaque ligne.
    Retourne un DataFrame avec :
        - id_ligne
        - moyenne_passagers_jour
    """
    pipeline = [
        {"$unwind": "$arrets"},
        {"$unwind": "$arrets.horaires"},
        {
            "$project": {
                "_id": 0,
                "id_ligne": "$id_ligne",
                "jour": {
                    "$substrBytes": ["$arrets.horaires.heure_prevue", 0, 10]
                },
                "passagers_estimes": "$arrets.horaires.passagers_estimes",
            }
        },
        {
            "$group": {
                "_id": {"id_ligne": "$id_ligne", "jour": "$jour"},
                "total_passagers_jour": {"$sum": "$passagers_estimes"},
            }
        },
        {
            "$group": {
                "_id": "$_id.id_ligne",
                "moyenne_passagers_jour": {"$avg": "$total_passagers_jour"},
            }
        },
        {
            "$project": {
                "_id": 0,
                "id_ligne": "$_id",
                "moyenne_passagers_jour": 1,
            }
        },
        {"$sort": {"moyenne_passagers_jour": -1}},
    ]

    df = aggregate_to_df(db.lignes, pipeline)
    if df.empty:
        return df
    return df[["id_ligne", "moyenne_passagers_jour"]]

def query_C_mongo(db) -> pd.DataFrame:
    """
    Requ√™te C (MongoDB).
    SQL: nom_ligne, nb_trafic_avec_incident, nb_total_trafic, taux_incident_pourcent
    """
    pipeline = [
        {
            "$unwind": {
                "path": "$trafic",
                "preserveNullAndEmptyArrays": True,
            },
        },
        {
            "$group": {
                "_id": {
                    "id_ligne": "$id_ligne",
                    "nom_ligne": "$nom_ligne",
                    "id_trafic": "$trafic.id_trafic",
                },
                "has_incident": {
                    "$max": {
                        "$cond": [
                            {
                                "$gt": [
                                    {
                                        "$size": {
                                            "$ifNull": [
                                                "$trafic.incidents",
                                                [],
                                            ],
                                        }
                                    },
                                    0,
                                ],
                            },
                            1,
                            0,
                        ],
                    },
                },
            },
        },
        {
            "$group": {
                "_id": "$_id.nom_ligne",
                "nb_trafic_avec_incident": {"$sum": "$has_incident"},
                "nb_total_trafic": {
                    "$sum": {
                        "$cond": [{"$ifNull": ["$_id.id_trafic", False]}, 1, 0],
                    },
                },
            },
        },
        {
            "$addFields": {
                "taux_incident_pourcent": {
                    "$cond": [
                        {"$eq": ["$nb_total_trafic", 0]},
                        0,
                        {
                            "$multiply": [
                                {
                                    "$divide": [
                                        "$nb_trafic_avec_incident",
                                        "$nb_total_trafic",
                                    ],
                                },
                                100,
                            ],
                        },
                    ],
                },
            },
        },
        {"$sort": {"taux_incident_pourcent": -1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": "$_id",
                "nb_trafic_avec_incident": 1,
                "nb_total_trafic": 1,
                "taux_incident_pourcent": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    cols = ["nom_ligne", "nb_trafic_avec_incident", "nb_total_trafic", "taux_incident_pourcent"]
    return df[cols] if not df.empty else df

def query_D_mongo(db) -> pd.DataFrame:
    """
    Requ√™te D (MongoDB) - ULTRA OPTIMIS√âE.
    Utilise 'vehicules_cache' et 'co2_moyen_ligne'.
    Complexit√©: O(L * V_per_line) au lieu de O(L * Arrets * Horaires).
    """
    pipeline = [
        # 1. On ne prend que les lignes qui ont une moyenne CO2 et des v√©hicules
        {
            "$match": {
                "co2_moyen_ligne": {"$exists": True, "$ne": None},
                "vehicules_cache": {"$exists": True, "$ne": []}
            }
        },
        # 2. On garde juste ce dont on a besoin
        {
            "$project": {
                "vehicules_cache": 1,
                "co2_moyen_ligne": 1
            }
        },
        # 3. On "sort" les v√©hicules de leur tableau
        {"$unwind": "$vehicules_cache"},
        
        # 4. Projection finale : le v√©hicule h√©rite du CO2 de sa ligne
        {
            "$project": {
                "_id": 0,
                "id_vehicule": "$vehicules_cache.id_vehicule",
                "immatriculation": "$vehicules_cache.immatriculation",
                "moyenne_co2": "$co2_moyen_ligne"
            }
        },
        # 5. Tri
        {"$sort": {"moyenne_co2": -1}}
    ]

    df = aggregate_to_df(db.lignes, pipeline)
    
    if df.empty:
        return pd.DataFrame(columns=["id_vehicule", "immatriculation", "moyenne_co2"])
        
    return df[["id_vehicule", "immatriculation", "moyenne_co2"]]

def query_E_mongo(db) -> pd.DataFrame:
    """
    Requ√™te E (MongoDB).
    SQL: nom, moyenne_bruit_db
    """
    pipeline = [
        {
            "$lookup": {
                "from": "capteurs",
                "localField": "arrets.id_arret",
                "foreignField": "arret.id_arret",
                "as": "caps",
            },
        },
        {"$unwind": "$caps"},
        {"$match": {"caps.type_capteur": "Bruit"}},
        {"$unwind": "$caps.mesures"},
        {
            "$group": {
                "_id": "$nom",
                "moyenne_bruit_db": {"$avg": "$caps.mesures.valeur"},
            },
        },
        {"$sort": {"moyenne_bruit_db": -1}},
        {"$limit": 5},
        {
            "$project": {
                "_id": 0,
                "nom": "$_id",
                "moyenne_bruit_db": 1,
            },
        },
    ]
    df = aggregate_to_df(db.quartiers, pipeline)
    return df[["nom", "moyenne_bruit_db"]] if not df.empty else df


def query_F_mongo(db) -> pd.DataFrame:
    """
    Requ√™te F (MongoDB).
    SQL: nom_ligne (DISTINCT)
    """
    pipeline = [
        {
            "$project": {
                "nom_ligne": 1,
                "has_big_delay": {
                    "$gt": [
                        {
                            "$size": {
                                "$filter": {
                                    "input": {"$ifNull": ["$trafic", []]},
                                    "as": "t",
                                    "cond": {
                                        "$gt": ["$$t.retard_minutes", 10],
                                    },
                                },
                            },
                        },
                        0,
                    ],
                },
                "has_incident": {
                    "$gt": [
                        {
                            "$size": {
                                "$filter": {
                                    "input": {"$ifNull": ["$trafic", []]},
                                    "as": "t",
                                    "cond": {
                                        "$gt": [
                                            {
                                                "$size": {
                                                    "$ifNull": [
                                                        "$$t.incidents",
                                                        [],
                                                    ]
                                                }
                                            },
                                            0,
                                        ],
                                    },
                                },
                            },
                        },
                        0,
                    ],
                },
            },
        },
        {
            "$match": {
                "has_big_delay": True,
                "has_incident": False,
            },
        },
        {
            "$project": {
                "_id": 0,
                "nom_ligne": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    return df[["nom_ligne"]] if not df.empty else df


def query_G_mongo(db) -> pd.DataFrame:
    """
    Requ√™te G (MongoDB).
    SQL: total_trajets, trajets_sans_retard, taux_ponctualite_global_pourcent
    """
    pipeline = [
        {"$unwind": "$trafic"},
        {
            "$group": {
                "_id": None,
                "total_trajets": {"$sum": 1},
                "trajets_sans_retard": {
                    "$sum": {
                        "$cond": [{"$eq": ["$trafic.retard_minutes", 0]}, 1, 0],
                    },
                },
            },
        },
        {
            "$addFields": {
                "taux_ponctualite_global_pourcent": {
                    "$cond": [
                        {"$eq": ["$total_trajets", 0]},
                        0,
                        {
                            "$multiply": [
                                {
                                    "$divide": [
                                        "$trajets_sans_retard",
                                        "$total_trajets",
                                    ],
                                },
                                100,
                            ],
                        },
                    ],
                },
            },
        },
        {
            "$project": {
                "_id": 0,
                "total_trajets": 1,
                "trajets_sans_retard": 1,
                "taux_ponctualite_global_pourcent": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    cols = ["total_trajets", "trajets_sans_retard", "taux_ponctualite_global_pourcent"]
    return df[cols] if not df.empty else df


def query_H_mongo(db) -> pd.DataFrame:
    """
    Requ√™te H (MongoDB).
    SQL: nom, nombre_arrets
    """
    pipeline = [
        {
            "$addFields": {
                "nombre_arrets": {"$size": {"$ifNull": ["$arrets", []]}},
            },
        },
        {"$sort": {"nombre_arrets": -1}},
        {
            "$project": {
                "_id": 0,
                "nom": 1,
                "nombre_arrets": 1,
            },
        },
    ]
    df = aggregate_to_df(db.quartiers, pipeline)
    return df[["nom", "nombre_arrets"]] if not df.empty else df


def query_I_mongo(db) -> pd.DataFrame:
    """
    Requ√™te I (MongoDB).
    SQL: nom_ligne, moyenne_retard, moyenne_co2
    """
    pipeline = [
        {
            "$addFields": {
                "moyenne_retard": {
                    "$cond": [
                        {
                            "$gt": [
                                {"$size": {"$ifNull": ["$trafic", []]}},
                                0,
                            ],
                        },
                        {"$avg": "$trafic.retard_minutes"},
                        0,
                    ],
                },
            },
        },
        {
            "$lookup": {
                "from": "capteurs",
                "localField": "id_ligne",
                "foreignField": "arret.id_ligne",
                "as": "capteurs_ligne",
            },
        },
        {
            "$addFields": {
                "mesures_co2": {
                    "$reduce": {
                        "input": "$capteurs_ligne",
                        "initialValue": [],
                        "in": {
                            "$cond": [
                                {"$eq": ["$$this.type_capteur", "CO2"]},
                                {
                                    "$concatArrays": [
                                        "$$value",
                                        {
                                            "$map": {
                                                "input": {
                                                    "$ifNull": [
                                                        "$$this.mesures",
                                                        [],
                                                    ]
                                                },
                                                "as": "m",
                                                "in": "$$m.valeur",
                                            }
                                        },
                                    ]
                                },
                                "$$value",
                            ],
                        },
                    },
                },
            },
        },
        {
            "$addFields": {
                "moyenne_co2": {
                    "$cond": [
                        {"$gt": [{"$size": "$mesures_co2"}, 0]},
                        {"$avg": "$mesures_co2"},
                        0,
                    ],
                },
            },
        },
        {"$sort": {"nom_ligne": 1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": 1,
                "moyenne_retard": 1,
                "moyenne_co2": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    return df[["nom_ligne", "moyenne_retard", "moyenne_co2"]] if not df.empty else df


def query_J_mongo(db) -> pd.DataFrame:
    """
    Requ√™te J (MongoDB).
    SQL: nom_ligne, moyenne_temperature
    """
    pipeline = [
        {"$match": {"type_capteur": "Temperature"}},
        {"$unwind": "$mesures"},
        {
            "$group": {
                "_id": "$arret.id_ligne",
                "moyenne_temperature": {"$avg": "$mesures.valeur"},
            },
        },
        {
            "$lookup": {
                "from": "lignes",
                "localField": "_id",
                "foreignField": "id_ligne",
                "as": "ligne",
            },
        },
        {"$unwind": "$ligne"},
        {"$sort": {"moyenne_temperature": -1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": "$ligne.nom_ligne",
                "moyenne_temperature": 1,
            },
        },
    ]
    df = aggregate_to_df(db.capteurs, pipeline)
    return df[["nom_ligne", "moyenne_temperature"]] if not df.empty else df

def query_K_mongo(db) -> pd.DataFrame:
    """
    Requ√™te K (MongoDB) - ULTRA OPTIMIS√âE.
    Utilise 'chauffeurs_cache' et 'stats_trafic' (pr√©-calcul√©).
    √âvite totalement de lire le tableau 'trafic'.
    """
    pipeline = [
        # 1. Filtre : Lignes avec chauffeurs et stats trafic existantes
        { 
            "$match": { 
                "stats_trafic": { "$exists": True },
                "chauffeurs_cache": { "$exists": True, "$ne": [] }
            } 
        },

        # 2. Projection l√©g√®re
        { 
            "$project": {
                "chauffeurs_cache": 1,
                "total_retard": "$stats_trafic.total_retard",
                "nb_trajets": "$stats_trafic.nb_trajets"
            }
        },

        # 3. On d√©roule la liste des chauffeurs (liste tr√®s courte)
        { "$unwind": "$chauffeurs_cache" },

        # 4. Groupement par chauffeur 
        # (Si un chauffeur est sur 2 lignes, on additionne les stats pr√©-calcul√©es)
        { 
            "$group": {
                "_id": "$chauffeurs_cache.nom_chauffeur",
                "cumul_retard": { "$sum": "$total_retard" },
                "cumul_trajets": { "$sum": "$nb_trajets" }
            }
        },

        # 5. Calcul final de la moyenne
        { 
            "$project": {
                "_id": 0,
                "nom": "$_id",
                "moyenne_retard_minutes": { 
                    "$cond": [
                        { "$eq": ["$cumul_trajets", 0] },
                        0,
                        { "$divide": ["$cumul_retard", "$cumul_trajets"] }
                    ]
                }
            }
        },

        # 6. Tri
        { "$sort": { "moyenne_retard_minutes": -1 } }
    ]

    df = aggregate_to_df(db.lignes, pipeline)
    return df[["nom", "moyenne_retard_minutes"]] if not df.empty else df

def query_L_mongo(db) -> pd.DataFrame:
    """
    Requ√™te L (MongoDB) - CORRIG√âE (D√âDUPLICATION V√âHICULES)
    Objectif : % Bus √©lectriques.
    SQL : nom_ligne, total_vehicules, nb_electriques, pourcentage_electrique
    """
    pipeline = [
        {"$match": {"type": "Bus"}},
        {"$unwind": "$arrets"},
        {"$unwind": "$arrets.horaires"},
        
        {"$match": {"arrets.horaires.vehicule.id_vehicule": {"$ne": None}}},

        # --- D√âDUPLICATION ---
        # On veut compter les V√âHICULES uniques par ligne, pas les horaires.
        {
            "$group": {
                "_id": {
                    "nom_ligne": "$nom_ligne",
                    "id_vehicule": "$arrets.horaires.vehicule.id_vehicule"
                },
                "type_vehicule": {"$first": "$arrets.horaires.vehicule.type_vehicule"}
            }
        },
        # ---------------------

        # Maintenant on compte les v√©hicules uniques
        {
            "$group": {
                "_id": "$_id.nom_ligne",
                "total_vehicules": {"$sum": 1},
                "nb_electriques": {
                    "$sum": {
                        "$cond": [
                            {"$eq": ["$type_vehicule", "Electrique"]},
                            1,
                            0,
                        ],
                    },
                },
            },
        },
        {
            "$addFields": {
                "pourcentage_electrique": {
                    "$cond": [
                        {"$eq": ["$total_vehicules", 0]},
                        0,
                        {
                            "$multiply": [
                                {
                                    "$divide": [
                                        "$nb_electriques",
                                        "$total_vehicules",
                                    ],
                                },
                                100,
                            ],
                        },
                    ],
                },
            },
        },
        {"$sort": {"pourcentage_electrique": -1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": "$_id",
                "total_vehicules": 1,
                "nb_electriques": 1,
                "pourcentage_electrique": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    cols = ["nom_ligne", "total_vehicules", "nb_electriques", "pourcentage_electrique"]
    return df[cols] if not df.empty else df
def query_M_mongo(db) -> pd.DataFrame:
    """
    Requ√™te M (MongoDB).
    SQL: id_capteur, latitude, longitude, moyenne_co2, niveau_pollution
    """
    pipeline = [
        {"$match": {"type_capteur": "CO2"}},
        {"$unwind": "$mesures"},
        {
            "$group": {
                "_id": {
                    "id_capteur": "$id_capteur",
                    "position": "$position",
                },
                "moyenne_co2": {"$avg": "$mesures.valeur"},
            },
        },
        {
            "$addFields": {
                "niveau_pollution": {
                    "$switch": {
                        "branches": [
                            {
                                "case": {"$gt": ["$moyenne_co2", 800]},
                                "then": "√âlev√©",
                            },
                            {
                                "case": {"$gt": ["$moyenne_co2", 450]},
                                "then": "Moyen",
                            },
                        ],
                        "default": "Faible",
                    },
                },
            },
        },
        {"$sort": {"moyenne_co2": -1}},
        {
            "$project": {
                "_id": 0,
                "id_capteur": "$_id.id_capteur",
                "latitude": {"$arrayElemAt": ["$_id.position.coordinates", 1]},
                "longitude": {"$arrayElemAt": ["$_id.position.coordinates", 0]},
                "moyenne_co2": 1,
                "niveau_pollution": 1,
            },
        },
    ]
    df = aggregate_to_df(db.capteurs, pipeline)
    cols = ["id_capteur", "latitude", "longitude", "moyenne_co2", "niveau_pollution"]
    return df[cols] if not df.empty else df


def query_N_mongo(db) -> pd.DataFrame:
    """
    Requ√™te N (MongoDB).
    SQL: nom_ligne, type, frequentation_moyenne, categorie_frequentation
    """
    pipeline = [
        {
            "$addFields": {
                "categorie_frequentation": {
                    "$switch": {
                        "branches": [
                            {
                                "case": {
                                    "$gt": [
                                        "$frequentation_moyenne",
                                        2000,
                                    ],
                                },
                                "then": "Haute Fr√©quentation",
                            },
                            {
                                "case": {
                                    "$gt": [
                                        "$frequentation_moyenne",
                                        1000,
                                    ],
                                },
                                "then": "Moyenne Fr√©quentation",
                            },
                        ],
                        "default": "Basse Fr√©quentation",
                    },
                },
            },
        },
        {"$sort": {"frequentation_moyenne": -1}},
        {
            "$project": {
                "_id": 0,
                "nom_ligne": 1,
                "type": 1,
                "frequentation_moyenne": 1,
                "categorie_frequentation": 1,
            },
        },
    ]
    df = aggregate_to_df(db.lignes, pipeline)
    cols = ["nom_ligne", "type", "frequentation_moyenne", "categorie_frequentation"]
    return df[cols] if not df.empty else df

QUERY_MONGO_FUNCS: Dict[str, Callable] = {
    "A": query_A_mongo,
    "B": query_B_mongo,
    "C": query_C_mongo,
    "D": query_D_mongo,
    "E": query_E_mongo,
    "F": query_F_mongo,
    "G": query_G_mongo,
    "H": query_H_mongo,
    "I": query_I_mongo,
    "J": query_J_mongo,
    "K": query_K_mongo,
    "L": query_L_mongo,
    "M": query_M_mongo,
    "N": query_N_mongo,
}


def executer_toutes_les_requetes_mongo() -> Dict[str, pd.DataFrame]:
    """
    Ex√©cute les requ√™tes A -> N sur la base MongoDB Paris2055 et
    sauvegarde les r√©sultats au format CSV.

    Retour
    ------
    dict[str, pandas.DataFrame]
        Dictionnaire associant le code de requ√™te √† son DataFrame
        r√©sultat. En cas d'erreur globale de connexion, toutes les
        entr√©es contiendront un DataFrame avec une colonne 'erreur'.
    """
    client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=2000)
    resultats: Dict[str, pd.DataFrame] = {}

    try:
        client.admin.command("ping")

        if MONGO_DB_NAME not in client.list_database_names():
            error_msg = (
                f"La base '{MONGO_DB_NAME}' n'existe pas. "
                "Lancez la migration (Partie 2)."
            )
            error_df = pd.DataFrame([{"erreur": error_msg}])
            return {code: error_df for code in QUERY_MONGO_FUNCS.keys()}

        db = client[MONGO_DB_NAME]
        for code, func in QUERY_MONGO_FUNCS.items():
            try:
                df = func(db)
            except Exception as exc:
                df = pd.DataFrame(
                    [{"erreur": f"Erreur requ√™te: {str(exc)}"}],
                )

            enregistrer_resultats_csv(
                DOSSIER_MONGO_CSV,
                f"resultat_req_{code.lower()}.csv",
                df,
            )
            resultats[code] = df

    except Exception as exc:
        err_msg = f"Impossible de se connecter √† MongoDB : {str(exc)}"
        print(f"[ERREUR] {err_msg}")
        error_df = pd.DataFrame([{"erreur": err_msg}])
        return {code: error_df for code in QUERY_MONGO_FUNCS.keys()}
    finally:
        client.close()

    return resultats

def render_partie_3_mongo(tab) -> None:
    """
    Affiche la Partie 3 : ex√©cution et visualisation des requ√™tes MongoDB.

    Param√®tres
    ----------
    tab :
        Conteneur Streamlit (onglet) dans lequel les √©l√©ments sont rendus.
    """
    with tab:
        st.subheader("Partie 3 : Requ√™tes MongoDB")

        server_ok, db_ok = check_connexion_details()

        if not server_ok:
            st.error(
                f"‚ùå Impossible de se connecter au serveur MongoDB sur {MONGO_URI}",
            )
        elif not db_ok:
            st.warning(
                f"‚ö†Ô∏è La base '{MONGO_DB_NAME}' n'existe pas encore. "
                "Lancez la migration en Partie 2.",
            )
        else:
            st.success(
                f"‚úÖ Serveur connect√© et base '{MONGO_DB_NAME}' d√©tect√©e.",
            )

        st.markdown("---")

        mongo_queries_executed = st.session_state.get(
            "queries_mongo_executed",
            False,
        )
        resultats_mongo: Dict[str, pd.DataFrame] = st.session_state.get(
            "resultats_mongo",
            {},
        )

        btn_disabled = not (server_ok and db_ok)

        if st.button(
            "Executer Requetes MongoDB",
            key="btn_mongo_run",
            disabled=btn_disabled,
        ):
            with st.spinner("Ex√©cution des requ√™tes MongoDB..."):
                resultats_mongo = executer_toutes_les_requetes_mongo()
            st.session_state["resultats_mongo"] = resultats_mongo
            st.session_state["queries_mongo_executed"] = True
            mongo_queries_executed = True
            st.success("‚úÖ Requ√™tes MongoDB termin√©es.")

        if not mongo_queries_executed:
            st.info(
                "Clique sur ¬´ Executer Requetes MongoDB ¬ª "
                "pour lancer les requ√™tes.",
            )
            return

        st.markdown("### R√©sultats d√©taill√©s des requ√™tes MongoDB")

        for code, objectif in st.session_state["requetes_objectifs"].items():
            df = resultats_mongo.get(code)
            with st.expander(
                f"Requ√™te {code} ‚Äì {objectif}",
                expanded=False,
            ):
                st.markdown(f"**Objectif :** {objectif}")
                if df is None:
                    st.warning("Aucun r√©sultat pour cette requ√™te.")
                elif df.empty:
                    st.info(
                        "La requ√™te n'a retourn√© aucun enregistrement.",
                    )
                else:
                    st.dataframe(
                        df.style.set_properties(
                            **{"text-align": "left"},
                        ),
                        width="content",
                    )
# =====================================================================
# Partie 4 : DASHBOARDS ET CARTOGRAPHIE
# =====================================================================

def render_partie_4_streamlit(tab) -> None:
    """
    Affiche la Partie 4 : espace r√©serv√© pour des dashboards et cartes.

    Param√®tres
    ----------
    tab :
        Conteneur Streamlit (onglet) dans lequel les √©l√©ments sont rendus.
    """
    with tab:
        st.subheader("Partie 4 : Tableau de bord et cartographie")
        st.info(
            "Espace r√©serv√© pour des visualisations suppl√©mentaires "
            "directement √† partir des donn√©es de Paris 2055 "
            "(cartes, dashboards, etc.).",
        )

# =====================================================================
# Partie 5 : COMPARAISON SQL vs MONGODB
# =====================================================================

def comparer_dataframes_souple(df1: pd.DataFrame, df2: pd.DataFrame) -> tuple[str, str]:
    """
    Compare deux DataFrames de mani√®re souple pour valider la migration.
    
    Retourne:
    - Un statut (ic√¥ne).
    - Un message explicatif.
    """
    if df1 is None or df2 is None:
        return "‚ùå", "Un des r√©sultats est manquant."
    
    if df1.empty and df2.empty:
        return "‚úÖ", "Les deux r√©sultats sont vides (coh√©rent)."
        
    if df1.empty or df2.empty:
        return "‚ùå", f"Disparit√© : SQL a {len(df1)} lignes, Mongo a {len(df2)} lignes."

    # 1. Comparaison du nombre de lignes
    if len(df1) != len(df2):
        diff = abs(len(df1) - len(df2))
        return "‚ö†Ô∏è", f"Diff√©rence de taille : {len(df1)} (SQL) vs {len(df2)} (Mongo). √âcart : {diff}."

    # 2. Comparaison du nombre de colonnes
    if len(df1.columns) != len(df2.columns):
        return "‚ö†Ô∏è", f"Colonnes diff√©rentes : {list(df1.columns)} vs {list(df2.columns)}."

    # 3. Tentative de comparaison stricte des valeurs (avec tol√©rance pour les arrondis)
    try:
        # On trie les donn√©es pour s'assurer qu'elles sont dans le m√™me ordre
        # On suppose que la premi√®re colonne est la cl√© de tri (ex: nom_ligne)
        col_sort_1 = df1.columns[0]
        col_sort_2 = df2.columns[0]
        
        df1_sorted = df1.sort_values(by=col_sort_1).reset_index(drop=True)
        df2_sorted = df2.sort_values(by=col_sort_2).reset_index(drop=True)

        # On normalise les noms de colonnes pour la comparaison (ignorer casse)
        df1_sorted.columns = [c.lower() for c in df1_sorted.columns]
        df2_sorted.columns = [c.lower() for c in df2_sorted.columns]

        pd.testing.assert_frame_equal(
            df1_sorted, 
            df2_sorted, 
            check_dtype=False, # Ignore int vs float
            check_exact=False, # Tol√®re les erreurs d'arrondi minimes
            rtol=1e-3 # Tol√©rance relative de 0.1%
        )
        return "‚úÖ", "Contenu identique (valeurs et dimensions)."
    except AssertionError as e:
        # Si c'est juste une histoire de noms de colonnes ou de types, on consid√®re que c'est acceptable
        return "‚ö†Ô∏è", "Dimensions OK, mais valeurs l√©g√®rement diff√©rentes (arrondis ou types)."
    except Exception as e:
        return "‚ùå", f"Erreur lors de la comparaison : {str(e)}"


def render_partie_5_comparaison(tab) -> None:
    """
    Affiche la Partie 5 : Comparaison c√¥te √† c√¥te des r√©sultats SQL et MongoDB.
    """
    with tab:
        st.subheader("Partie 5 : Validation de la Migration (SQL vs NoSQL)")
        st.markdown(
            "Cet onglet permet de v√©rifier si les requ√™tes MongoDB renvoient "
            "bien les m√™mes donn√©es m√©tier que les requ√™tes SQL d'origine."
        )

        # V√©rification que les caches sont charg√©s
        sql_ready = st.session_state.get("queries_sql_executed", False)
        mongo_ready = st.session_state.get("queries_mongo_executed", False)

        if not sql_ready or not mongo_ready:
            st.warning("‚ö†Ô∏è Veuillez ex√©cuter les requ√™tes de la **Partie 1** (SQL) et de la **Partie 3** (MongoDB) pour voir la comparaison.")
            return

        st.markdown("---")

        res_sql = st.session_state["resultats_sql"]
        res_mongo = st.session_state["resultats_mongo"]
        objectifs = st.session_state["requetes_objectifs"]

        # Compteurs pour le r√©sum√©
        total_ok = 0
        total_queries = len(objectifs)

        for code, objectif in objectifs.items():
            df_sql = res_sql.get(code)
            df_mongo = res_mongo.get(code)

            # Calcul du statut
            icon, message = comparer_dataframes_souple(df_sql, df_mongo)
            if icon == "‚úÖ":
                total_ok += 1

            # Affichage dans un expander
            with st.expander(f"{icon} Requ√™te {code} : {objectif[:60]}..."):
                st.caption(f"**Objectif :** {objectif}")
                
                # Message de statut
                if icon == "‚úÖ":
                    st.success(f"R√©sultat : {message}")
                elif icon == "‚ö†Ô∏è":
                    st.warning(f"R√©sultat : {message}")
                else:
                    st.error(f"R√©sultat : {message}")

                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.markdown("**1. R√©sultat SQL (Source)**")
                    if df_sql is not None and not df_sql.empty:
                        st.dataframe(df_sql, width='stretch', height=200)
                        st.caption(f"Lignes : {len(df_sql)} | Colonnes : {len(df_sql.columns)}")
                    else:
                        st.info("Vide ou erreur.")

                with col_b:
                    st.markdown("**2. R√©sultat MongoDB (Cible)**")
                    if df_mongo is not None and not df_mongo.empty:
                        st.dataframe(df_mongo, width='stretch', height=200)
                        st.caption(f"Lignes : {len(df_mongo)} | Colonnes : {len(df_mongo.columns)}")
                    else:
                        st.info("Vide ou erreur.")

        st.markdown("---")
        
        # Score final de validation
        score = int((total_ok / total_queries) * 100)
        if score == 100:
            st.balloons()
            st.success(f"üèÜ Migration valid√©e √† 100% ! ({total_ok}/{total_queries} requ√™tes identiques)")
        elif score > 80:
            st.success(f"‚úÖ Migration valid√©e √† {score}% ({total_ok}/{total_queries} requ√™tes identiques)")
        else:
            st.error(f"‚ùå Attention : Seulement {score}% de correspondance ({total_ok}/{total_queries}). V√©rifiez vos pipelines.")

# =====================================================================
# Partie 6 : ASSISTANT IA GROQ / LLAMA3
# =====================================================================
def interroger_groq(question: str) -> tuple[Optional[Dict], Optional[str]]:
    # --- MODIFICATION ICI : On r√©cup√®re la cl√© depuis la session ou l'input ---
    api_key = st.session_state.get("groq_api_key", "")
    
    if not api_key or "gsk_" not in api_key:
        return None, "Cl√© API Groq manquante ou invalide. V√©rifiez la sidebar."

    # On passe la cl√© dynamique au client
    client = Groq(api_key=api_key)

    try:
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": SCHEMA_CONTEXT},
                {"role": "user", "content": f"La question est : {question}"},
            ],
            temperature=0,
            stream=False,
            response_format={"type": "json_object"},
        )

        response_content = completion.choices[0].message.content
        data = json.loads(response_content)
        return data, None

    except Exception as exc:
        return None, str(exc)
    
def render_partie_6_ia(tab) -> None:
    """
    Affiche la Partie 6 : assistant IA pilotant la g√©n√©ration de requ√™tes
    MongoDB via Groq / Llama 3.
    """
    QUESTION_BUTTONS = [
        "la moyenne des retards (en minutes) pour chaque ligne de transport.",

        "le nombre moyen de passagers transport√©s par jour pour chaque ligne.",

        "le taux d'incidents (en pourcentage) pour chaque ligne, bas√© sur le nombre de trajets ayant signal√© un incident.",

        "les 5 quartiers ayant la moyenne de niveau de bruit (en dB) la plus √©lev√©e, bas√©e sur les capteurs de bruit aux arr√™ts."
    ]

    # Initialisation de l'√©tat pour la r√©ponse JSON de l'IA
    if "ai_json_response" not in st.session_state:
        st.session_state["ai_json_response"] = None

    with tab:
        st.subheader("Partie 6 : Assistant IA ü§ñ (Powered by Groq/Llama3)")
        st.markdown(
            "Posez n'importe quelle question sur vos donn√©es. "
            "L'IA va g√©n√©rer la requ√™te MongoDB complexe pour vous.",
        )

        # Zone de saisie manuelle : on NE modifie jamais la cl√© 'ai_question_input' dans le code
        question = st.text_area(
            "üí¨ Posez votre question :",
            key="ai_question_input",
            height=70,
        )

        # Affichage du dernier JSON g√©n√©r√© par l'IA (si disponible)
        if st.session_state.get("ai_json_response"):
            with st.expander(
                f"Voir le dernier JSON g√©n√©r√© par l'IA "
                f"(Cible : {st.session_state['ai_json_response'].get('collection', 'N/A')})",
                expanded=True,
            ):
                st.code(
                    json.dumps(
                        st.session_state["ai_json_response"],
                        indent=2,
                        ensure_ascii=False,
                    ),
                    language="json",
                )

        col_btn, _ = st.columns([1, 3])

        # Variable locale qui d√©cidera si on lance l'IA dans ce run
        question_a_executer: Optional[str] = None

        # 1) Bouton principal : on utilise la question tap√©e dans la zone de texte
        if col_btn.button("‚ú® G√©n√©rer & Ex√©cuter", type="primary", key="btn_ia_run"):
            question_a_executer = question.strip()

        st.markdown("---")

        # 2) Boutons de questions rapides : on ex√©cute directement le texte du bouton
        st.markdown("### Questions fr√©quentes :")
        cols = st.columns(len(QUESTION_BUTTONS))
        for i, question_text in enumerate(QUESTION_BUTTONS):
            if cols[i].button(question_text, key=f"quick_q_{i}"):
                question_a_executer = question_text

        # Si aucun bouton n'a √©t√© cliqu√©, on s'arr√™te l√†
        if question_a_executer is None:
            return

        question_a_executer = question_a_executer.strip()
        if not question_a_executer:
            st.warning("Veuillez √©crire une question.")
            st.session_state["ai_json_response"] = None
            return

        st.markdown(f"**Question envoy√©e √† l'IA :** {question_a_executer}")

        # Appel √† Groq
        with st.spinner("L'IA analyse votre demande..."):
            result_ia, error = interroger_groq(question_a_executer)

        st.session_state["ai_json_response"] = result_ia

        if error:
            st.error(f"Erreur API/LLM : {error}")
            if "Cl√© API" in error:
                st.info(
                    "Allez sur https://console.groq.com pour avoir une cl√© gratuite !",
                )
            return

        collection_cible = result_ia.get("collection")
        pipeline: Optional[List] = result_ia.get("pipeline")

        if not pipeline:
            st.error(
                "‚ùå Requ√™te non comprise ou non pertinente pour la base de donn√©es. "
                "Veuillez poser une question concernant les lignes, capteurs ou quartiers de Paris 2055.",
            )
            return

        st.success("Requ√™te g√©n√©r√©e avec succ√®s !")

        # Ex√©cution MongoDB
        with st.spinner(
            f"Ex√©cution sur la collection '{collection_cible}'...",
        ):
            try:
                client = pymongo.MongoClient(MONGO_URI)
                db = client[MONGO_DB_NAME]

                if collection_cible not in db.list_collection_names():
                    st.error(
                        "Erreur : L'IA veut chercher dans "
                        f"'{collection_cible}' mais cette collection "
                        "n'existe pas.",
                    )
                    client.close()
                    return

                collection = db[collection_cible]
                results = list(collection.aggregate(pipeline))
                client.close()

                if results:
                    st.markdown(f"### üìä R√©sultats ({len(results)})")
                    df_res = pd.DataFrame(results)
                    if "_id" in df_res.columns:
                        df_res["_id"] = df_res["_id"].astype(str)
                    st.dataframe(df_res, width='stretch')
                else:
                    st.warning(
                        "La requ√™te est valide syntaxiquement, "
                        "mais aucun r√©sultat n'a √©t√© trouv√©.",
                    )
            except Exception as exc:
                st.error(f"Erreur lors de l'ex√©cution MongoDB : {exc}")

# =====================================================================
# MAIN STREAMLIT
# =====================================================================
def main() -> None:
    """
    Point d'entr√©e de l'application Streamlit Paris 2055.
    """
    st.set_page_config(
        page_title="Paris 2055 - Requ√™tes et Migration vers MongoDB",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    init_session_state()

    st.title("Paris 2055 - Requ√™tes et Migration vers MongoDB")

    with st.sidebar:
        # =================================================
        # 1. CONFIGURATION API (En premier)
        # =================================================
        st.header("üîë Configuration API")
        
        # Champ de texte connect√© au session_state
        new_key = st.text_input(
            label="Groq API Key",
            value=st.session_state["groq_api_key"],
            type="password", 
            help="Collez votre cl√© gsk_... ici. Elle sera utilis√©e pour les requ√™tes IA."
        )

        # Si l'utilisateur change la cl√©
        if new_key != st.session_state["groq_api_key"]:
            st.session_state["groq_api_key"] = new_key
            
            # 1. Mise √† jour en m√©moire (pour l'utilisation imm√©diate)
            os.environ["GROQ_API_KEY"] = new_key
            
            # 2. Mise √† jour du fichier .env physique (pour la persistance)
            dotenv_path = ".env"
            try:
                # set_key cr√©e le fichier s'il n'existe pas, ou met √† jour la ligne si elle existe
                set_key(dotenv_path, "GROQ_API_KEY", new_key)
                st.success("Cl√© sauvegard√©e dans .env ! ‚úÖ")
            except Exception as e:
                st.warning(f"Cl√© active mais non sauvegard√©e dans .env : {e}")
            
            time.sleep(1)
            st.rerun()

        st.markdown("---")

        # =================================================
        # 2. √âTAT DES BASES DE DONN√âES
        # =================================================
        st.header("üì° √âtat des Bases de Donn√©es")

        # --- A. SQLITE (SOURCE) ---
        if os.path.exists(DB_FILE):
            st.success("Source SQLite : **Trouv√©e**", icon="üìÑ")
        else:
            st.error("Source SQLite : **Introuvable**", icon="‚ùå")

        # --- B. MONGODB (CIBLE) ---
        server_ok, db_ok = check_connexion_details()

        if server_ok:
            st.success("Serveur MongoDB : **Connect√©**", icon="‚úÖ")
            
            if db_ok:
                # --- LOGIQUE D'INSPECTION DU CONTENU ---
                try:
                    # On ouvre une connexion temporaire pour compter
                    temp_client = pymongo.MongoClient(MONGO_URI)
                    temp_db = temp_client[MONGO_DB_NAME]
                    
                    # Liste des collections attendues
                    cols_to_check = ["lignes", "quartiers", "capteurs"]
                    details = []
                    is_empty = True
                    
                    for col_name in cols_to_check:
                        count = temp_db[col_name].count_documents({})
                        if count > 0:
                            is_empty = False
                            details.append(f"‚ñ™Ô∏è **{col_name}** : {count} docs \n")
                        else:
                            details.append(f"‚ñ™Ô∏è **{col_name}** : ‚ö†Ô∏è 0 doc \n")
                    
                    temp_client.close()

                    # Affichage conditionnel selon le contenu
                    if is_empty:
                        st.warning(f"Base '{MONGO_DB_NAME}' : **Vide**", icon="üì≠")
                    else:
                        st.success(f"Base '{MONGO_DB_NAME}' : **Remplie**", icon="üçÉ")
                    
                    # Affichage des d√©tails dans un petit menu d√©roulant
                    with st.expander("Voir le contenu"):
                        st.markdown("\n".join(details))

                except Exception:
                    st.warning("Base existante (Lecture impossible)", icon="‚ö†Ô∏è")
            else:
                st.warning(f"Base '{MONGO_DB_NAME}' : **Manquante**", icon="‚ùå")
        else:
            st.error("Serveur MongoDB : **D√©connect√©**", icon="‚ùå")

        st.markdown("---")

        # =================================================
        # 3. √âTAT DES CACHES (CSV)
        # =================================================
        st.header("üóÇÔ∏è √âtat des Caches (CSV)")

        # --- CACHE SQL ---
        if st.session_state.get("queries_sql_executed", False):
            st.success("R√©sultats SQL : **Charg√©s**", icon="‚úÖ")
        else:
            st.info("R√©sultats SQL : **Vides**", icon="‚ö™")

        # --- CACHE MONGODB ---
        if st.session_state.get("queries_mongo_executed", False):
            st.success("R√©sultats Mongo : **Charg√©s**", icon="‚úÖ")
        else:
            st.info("R√©sultats Mongo : **Vides**", icon="‚ö™")    
        
    # =================================================
    # CORPS PRINCIPAL (ONGLETS)
    # =================================================
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
        [
            "Partie 1 : SQL",
            "Partie 2 : Migration",
            "Partie 3 : Mongo",
            "Partie 4 : Dashboard",
            "Partie 5 : Comparaison",
            "Partie 6 : Assistant requ√™tes IA",
        ],
    )

    render_partie_1_sqlite(tab1)
    render_partie_2_migration(tab2)
    render_partie_3_mongo(tab3)
    render_partie_4_streamlit(tab4)
    render_partie_5_comparaison(tab5)
    render_partie_6_ia(tab6)

if __name__ == "__main__":
    main()