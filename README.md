# Paris 2055 : ETL & Analytics MongoDB ğŸš‡

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-4EA94B?style=for-the-badge&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![Groq AI](https://img.shields.io/badge/AI-Groq_Llama3.3-orange?style=for-the-badge)](https://groq.com/)

> **Projet pÃ©dagogique** - BUT 3 Sciences des DonnÃ©es (2025)

---

## ğŸ“– Description du Projet

**Paris 2055** est une application complÃ¨te d'**ETL** (Extract, Transform, Load) et d'**analyse de donnÃ©es** qui simule la migration d'un systÃ¨me de transport urbain futuriste depuis une architecture **relationnelle (SQLite)** vers une architecture **orientÃ©e documents (MongoDB)**.

### Contexte MÃ©tier

En 2055, le rÃ©seau de transport parisien gÃ©nÃ¨re des millions de donnÃ©es en temps rÃ©el :
- ğŸ“Š Mesures de capteurs environnementaux (**CO2**, **Bruit**, **TempÃ©rature**)
- ğŸšŒ Informations de **trafic** (retards, incidents, frÃ©quentation)
- ğŸ—ºï¸ DonnÃ©es **gÃ©ospatiales** (quartiers, arrÃªts, positions des capteurs)
- ğŸ‘¨â€âœˆï¸ DonnÃ©es opÃ©rationnelles (chauffeurs, vÃ©hicules, horaires)

### Objectif Technique

Migrer les donnÃ©es depuis un modÃ¨le **relationnel normalisÃ©** (14 tables SQLite) vers un modÃ¨le **dÃ©normalisÃ© orientÃ© documents** (3 collections MongoDB) pour :
- âœ… Supporter la **volumÃ©trie** croissante (millions de mesures)
- âœ… Optimiser les **requÃªtes analytiques** complexes
- âœ… Tirer parti de **MongoDB** (agrÃ©gations, gÃ©ospatial, flexibilitÃ© du schÃ©ma)
- âœ… IntÃ©grer un **assistant IA** pour gÃ©nÃ©rer des requÃªtes en langage naturel

---

## ğŸ› ï¸ Technologies & BibliothÃ¨ques

| Technologie | Version | Utilisation |
|-------------|---------|-------------|
| **Python** | 3.10+ | Langage principal |
| **Streamlit** | 1.x | Interface web interactive (6 onglets) |
| **Pandas** | 2.x | Manipulation de DataFrames pour l'ETL |
| **PyMongo** | 4.x | Client MongoDB (connexion, requÃªtes, agrÃ©gations) |
| **SQLite3** | (std lib) | AccÃ¨s Ã  la base relationnelle source |
| **Groq SDK** | Latest | API Llama 3.3-70B pour l'assistant IA |
| **python-dotenv** | Latest | Gestion des variables d'environnement (.env) |

---

## ğŸ“¦ Installation

### PrÃ©requis

1. **Python 3.10 ou supÃ©rieur**
   ```bash
   python --version  # VÃ©rifiez votre version
   ```

2. **MongoDB** (local ou Atlas)
   - **Option A (Local)** : Installez MongoDB Community Server
   - **Option B (Cloud)** : CrÃ©ez un cluster gratuit sur [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)

3. **ClÃ© API Groq** (pour l'assistant IA)
   - CrÃ©ez un compte sur [console.groq.com](https://console.groq.com/)
   - GÃ©nÃ©rez une clÃ© API (format : `gsk_...`)

### Ã‰tapes d'Installation

```bash
# 1. Cloner le dÃ©pÃ´t (ou tÃ©lÃ©charger le projet)
git clone https://github.com/qurnt1/mongoDB_ST_QC_CA_JF.git
cd mongoDB_ST_QC_CA_JF

# 2. CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv

# 3. Activer l'environnement virtuel
.\venv\Scripts\activate

# 4. CrÃ©er le fichier .env
@("GROQ_API_KEY=''", "MONGO_URI=''") | Set-Content .env

# 5. Installer les dÃ©pendances
pip install -r requirements.txt
```

**macOs, Linux**

```bash
# 1. Cloner le dÃ©pÃ´t (ou tÃ©lÃ©charger le projet)
git clone https://github.com/qurnt1/mongoDB_ST_QC_CA_JF.git
cd mongoDB_ST_QC_CA_JF

# 2. CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv

# 3. Activer l'environnement virtuel
source venv/bin/activate

# 4. CrÃ©er le fichier .env
echo "GROQ_API_KEY=''" > .env && echo "MONGO_URI=''" >> .env

# 5. Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration (.env)

Modifiez / crÃ©ez le fichier `.env` Ã  la racine du projet :

```env
# ClÃ© API Groq (obligatoire pour l'assistant IA - Partie 6)
GROQ_API_KEY="gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# URI MongoDB (optionnel, valeur par dÃ©faut : mongodb://127.0.0.1:27017/)
# Exemples :
# Local : MONGO_URI="mongodb://localhost:27017/"
# Atlas : MONGO_URI="mongodb+srv://user:password@cluster.mongodb.net/"
MONGO_URI="mongodb://127.0.0.1:27017/"
```

> **Note** : L'URI MongoDB peut Ã©galement Ãªtre modifiÃ©e directement dans l'interface Streamlit (sidebar â†’ Config API & DB).

---

## ğŸ—ï¸ Architecture

### SchÃ©ma de DonnÃ©es Source (SQLite)

La base `paris2055.sqlite` contient **14 tables relationnelles** :

| Table | Description | ClÃ©s Principales |
|-------|-------------|------------------|
| `Ligne` | Lignes de transport (Bus, MÃ©tro, Tramway) | `id_ligne`, `nom_ligne`, `type` |
| `Arret` | ArrÃªts de transport | `id_arret`, `id_ligne`, `latitude`, `longitude` |
| `Horaire` | Passages prÃ©vus | `id_horaire`, `id_arret`, `id_vehicule`, `heure_prevue`, `passagers_estimes` |
| `Trafic` | Ã‰vÃ©nements de circulation | `id_trafic`, `id_ligne`, `retard_minutes` |
| `Incident` | Incidents signalÃ©s | `id_incident`, `id_trafic`, `description`, `gravite` |
| `Capteur` | Capteurs environnementaux | `id_capteur`, `id_arret`, `type_capteur`, `latitude`, `longitude` |
| `Mesure` | RelevÃ©s de capteurs | `id_mesure`, `id_capteur`, `valeur`, `horodatage`, `unite` |
| `Quartier` | Quartiers parisiens | `id_quartier`, `nom`, `geojson` (polygone) |
| `ArretQuartier` | Relation arrÃªt-quartier | `id_arret`, `id_quartier` |
| `Vehicule` | Flotte de vÃ©hicules | `id_vehicule`, `id_ligne`, `id_chauffeur`, `type_vehicule`, `immatriculation` |
| `Chauffeur` | Conducteurs | `id_chauffeur`, `nom` |

### Processus ETL (Extract, Transform, Load)

#### 1. **Extract** - Chargement optimisÃ© depuis SQLite

```python
# Fonction : load_tables()
# Optimisation : Chargement par CHUNKS pour Ã©viter les erreurs "Out of Memory"
```

- **Tables petites** (`Ligne`, `Arret`, `Quartier`, `Vehicule`, etc.) : Chargement standard
- **Tables volumineuses** (`Horaire`, `Mesure`, `Trafic`, `Incident`) : Chargement par chunks de 50 000 lignes

#### 2. **Transform** - Construction du modÃ¨le document

```python
# Fonctions : build_lignes_docs(), build_quartiers_docs(), build_capteurs_docs()
# Optimisation : Utilisation de itertuples() au lieu de iterrows() (10x plus rapide)
```

**StratÃ©gies de transformation :**

- **DÃ©normalisation** : Regroupement des donnÃ©es liÃ©es en sous-documents imbriquÃ©s
- **Caches prÃ©-calculÃ©s** : Ajout de champs dÃ©rivÃ©s pour optimiser les requÃªtes :
  - `vehicules_cache` : Liste des vÃ©hicules de la ligne (Ã©vite un $lookup)
  - `chauffeurs_cache` : Liste des chauffeurs affectÃ©s Ã  la ligne
  - `stats_trafic` : Statistiques prÃ©-agrÃ©gÃ©es (total_retard, nb_trajets, moyenne_precalc)
  - `co2_moyen_ligne` : Moyenne de CO2 calculÃ©e par ligne

**Exemple de transformation :**

```
SQLite (ModÃ¨le Relationnel)          MongoDB (ModÃ¨le Document)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ligne   â”‚                         â”‚         lignes               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id_ligne â”‚â”€â”€â”                      â”‚ id_ligne                     â”‚
â”‚ nom      â”‚  â”‚                      â”‚ nom_ligne                    â”‚
â”‚ type     â”‚  â”‚                      â”‚ type                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚ co2_moyen_ligne (cache)      â”‚
              â”‚                      â”‚ stats_trafic: {              â”‚
              â”‚                      â”‚   total_retard,              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚   nb_trajets,                â”‚
â”‚  Arret   â”‚â”€â”€â”˜                      â”‚   moyenne_precalc            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”‚ }                            â”‚
â”‚ id_arret â”‚â”€â”€â”                      â”‚ arrets: [                    â”‚
â”‚ nom      â”‚  â”‚                      â”‚   {                          â”‚
â”‚ lat/lon  â”‚  â”‚                      â”‚     id_arret,                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚     nom,                     â”‚
              â”‚                      â”‚     latitude,                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚     longitude,               â”‚
â”‚ Horaire  â”‚â”€â”€â”˜  ==TRANSFORM==>     â”‚     horaires: [              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”‚       { heure_prevue,        â”‚
â”‚ heure    â”‚                         â”‚         passagers_estimes }  â”‚
â”‚ passagersâ”‚                         â”‚     ],                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚     capteurs_ids: [12, 34]   â”‚
                                     â”‚   }                          â”‚
                                     â”‚ ],                           â”‚
                                     â”‚ trafic: [                    â”‚
                                     â”‚   { retard_minutes,          â”‚
                                     â”‚     incidents: [...] }       â”‚
                                     â”‚ ]                            â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. **Load** - Insertion dans MongoDB

```python
# Fonction : migrer_sqlite_vers_mongo()
# Optimisation : Insertion par batch de 25 000 documents (insert_many)
```

**Collections MongoDB crÃ©Ã©es :**

| Collection | Documents | Structure Principale |
|------------|-----------|---------------------|
| `lignes` | ~100 docs | Lignes avec arrÃªts, horaires, trafic, incidents, caches |
| `quartiers` | ~100 docs | Quartiers avec gÃ©omÃ©trie GeoJSON (Polygon) et arrÃªts |
| `capteurs` | ~4100 docs | Capteurs avec position GeoJSON (Point) et mesures |

**Index crÃ©Ã©s automatiquement :**

```javascript
// Index standards
db.lignes.createIndex({ id_ligne: 1 })
db.lignes.createIndex({ nom_ligne: 1 })
db.capteurs.createIndex({ type_capteur: 1 })

// Index gÃ©ospatiaux (2dsphere) pour les requÃªtes de proximitÃ©
db.quartiers.createIndex({ geom: "2dsphere" })
db.capteurs.createIndex({ position: "2dsphere" })
```

---

## â–¶ï¸ Utilisation

### Lancement de l'Application

```bash
streamlit run app.py
```

L'application s'ouvre automatiquement dans votre navigateur Ã  l'adresse **http://localhost:8501**

### Interface Streamlit (6 Onglets)

#### **Partie 1 : RequÃªtes SQL (Legacy)**
- ExÃ©cution de **14 requÃªtes mÃ©tier** sur la base SQLite source
- RequÃªtes couvrant : moyennes de retards, taux d'incidents, frÃ©quentation, Ã©missions CO2, niveaux sonores, etc.
- Export des rÃ©sultats au format **CSV** (cache persistant)

#### **Partie 2 : ETL & Migration**
- Bouton **"Lancer Migration"** pour dÃ©clencher le processus complet :
  1. Nettoyage des anciennes collections MongoDB
  2. Chargement des tables SQLite (optimisÃ© par chunks)
  3. Construction des documents (avec caches et transformations)
  4. Sauvegarde JSON locale (backup)
  5. Insertion dans MongoDB (par batch)
  6. CrÃ©ation des index
- **Logs en temps rÃ©el** affichant la progression

#### **Partie 3 : Analytics NoSQL**
- ExÃ©cution de **14 requÃªtes Ã©quivalentes** sur MongoDB
- Utilisation de **pipelines d'agrÃ©gation** ($match, $group, $unwind, $lookup, etc.)
- Exploitation des **caches prÃ©-calculÃ©s** (vehicules_cache, stats_trafic)
- Export CSV des rÃ©sultats

#### **Partie 4 : Tableau de Bord**
- Espace rÃ©servÃ© pour des visualisations avancÃ©es (cartes, graphiques interactifs)

#### **Partie 5 : Validation & Comparaison**
- Comparaison automatisÃ©e **SQL vs MongoDB** (ligne par ligne)
- Scoring de validation (pourcentage de requÃªtes identiques)
- Affichage cÃ´te Ã  cÃ´te des rÃ©sultats pour chaque requÃªte

#### **Partie 6 : Assistant IA ğŸ§ **
- **ModÃ¨le utilisÃ©** : Llama 3.3-70B Versatile (via API Groq)
- **FonctionnalitÃ©s** :
  - Posez une question en **langage naturel** (franÃ§ais)
  - L'IA gÃ©nÃ¨re un **pipeline MongoDB** en JSON
  - ExÃ©cution automatique du pipeline
  - Analyse textuelle des rÃ©sultats par l'IA
- **Exemples de questions** :
  - _"Quelle est la moyenne des retards pour chaque ligne de transport ?"_
  - _"Quels sont les 5 quartiers les plus bruyants ?"_
  - _"Classe les capteurs CO2 par niveau de pollution"_

---

## ğŸš¦ Parcours Utilisateur RecommandÃ©

1. **VÃ©rifier l'Ã©tat du systÃ¨me** (sidebar)
   - SQLite : âœ… Ready
   - MongoDB : ğŸš« Offline/Empty

2. **Partie 1** : ExÃ©cuter les requÃªtes SQL pour gÃ©nÃ©rer les donnÃ©es de rÃ©fÃ©rence

3. **Partie 2** : Lancer la migration SQLite â†’ MongoDB
   - DurÃ©e estimÃ©e : 30-60 secondes (selon la machine)
   - VÃ©rifier que MongoDB passe Ã  âœ… Ready dans la sidebar

4. **Partie 3** : ExÃ©cuter les requÃªtes MongoDB

5. **Partie 5** : Comparer les rÃ©sultats (validation automatique)
   - Objectif : 100% de correspondance

6. **Partie 6** : Tester l'assistant IA avec vos propres questions

---

## âš™ï¸ FonctionnalitÃ©s AvancÃ©es

### SystÃ¨me de Cache Intelligent

```python
@st.cache_data(show_spinner=False)
def executer_toutes_les_requetes():
    # Calculs SQL cachÃ©s en mÃ©moire par Streamlit
    ...
```

- **Cache Streamlit** : Les requÃªtes SQL et MongoDB ne sont exÃ©cutÃ©es qu'une seule fois
- **Cache disque** : Les rÃ©sultats sont sauvegardÃ©s en CSV
- **Restauration automatique** : Au red\u00e9marrage, l'app recharge les CSV existants

### Gestion d'Ã‰tat (Session State)

```python
st.session_state["resultats_sql"]      # RÃ©sultats des requÃªtes SQL
st.session_state["resultats_mongo"]    # RÃ©sultats des requÃªtes MongoDB
st.session_state["migration_logs"]     # Journal de la derniÃ¨re migration
st.session_state["mongo_uri"]          # URI MongoDB modifiable via l'interface
st.session_state["groq_api_key"]       # ClÃ© API Groq modifiable via l'interface
```

### Sidebar "Admin"

- **Ã‰tat du SystÃ¨me** : Indicateurs visuels (SQLite/MongoDB)
- **Ã‰tat des Caches** : SQL/Mongo chargÃ©s ou vides
- **Config API & DB** : Modification dynamique de l'URI MongoDB et de la clÃ© Groq
- **Danger Zone** ğŸ§¨ : RÃ©initialisation complÃ¨te (suppression MongoDB + CSV + caches)

### Assistant IA - Mode JSON Strict

```python
response_format={"type": "json_object"}  # Force une rÃ©ponse JSON valide
```

L'IA gÃ©nÃ¨re systÃ©matiquement un objet JSON structurÃ© :

```json
{
  "collection": "lignes",
  "pipeline": [
    { "$match": { "type": "Bus" } },
    { "$group": { "_id": "$nom_ligne", "count": { "$sum": 1 } } }
  ],
  "explication": "Cette requÃªte compte le nombre de bus par ligne"
}
```

---

## ğŸ“š Structure du Projet

```
mongoDB_ST_QC_CA_JF/
â”‚
â”œâ”€â”€ app.py                          # Application principale (point d'entrÃ©e)
â”œâ”€â”€ requirements.txt                # DÃ©pendances Python
â”œâ”€â”€ .env                            # Variables d'environnement (crÃ©Ã© par l'utilisateur)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ context_prompt.txt          # Prompt systÃ¨me pour l'IA (schÃ©ma MongoDB)
â”‚   â”‚
â”‚   â”œâ”€â”€ sqlite/
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ paris2055.sqlite    # Base SQLite source (relationnelle)
â”‚   â”‚   â””â”€â”€ resultats_requetes_sqlite/  # CSV des rÃ©sultats SQL
â”‚   â”‚
â”‚   â””â”€â”€ mongodb/
â”‚       â”œâ”€â”€ collections/            # Backup JSON des collections MongoDB
â”‚       â””â”€â”€ resultats_requetes_mongodb/  # CSV des rÃ©sultats MongoDB
â”‚
â””â”€â”€ README.md                       # Ce fichier
```

---

## ğŸ“Š Statistiques

| MÃ©trique | Valeur |
|----------|--------|
| Lignes de code | ~3 260 lignes (app.py) |
| Tables SQLite | 14 tables |
| Collections MongoDB | 3 collections |
| RequÃªtes mÃ©tier | 14 requÃªtes (SQL + MongoDB) |
| Optimisations ETL | 4 caches prÃ©-calculÃ©s |
| ModÃ¨le IA | Llama 3.3-70B (70 milliards de paramÃ¨tres) |

---

## ğŸ“ Contexte PÃ©dagogique

Ce projet a Ã©tÃ© dÃ©veloppÃ© dans le cadre du **BUT 3 Informatique - Parcours Sciences des DonnÃ©es** (2025) pour illustrer :

- âœ… La **migration de schÃ©ma** relationnel â†’ NoSQL
- âœ… Les **optimisations ETL** (chunks, vectorisation Pandas, caches)
- âœ… Les **pipelines d'agrÃ©gation MongoDB** (Ã©quivalents SQL complexes)
- âœ… L'utilisation de **modÃ¨les d'IA gÃ©nÃ©ratifs** pour le requÃªtage en langage naturel
- âœ… La construction d'une **interface web interactive** avec Streamlit

---

## ğŸ“œ Licence

Projet pÃ©dagogique - BUT 3 Sciences des DonnÃ©es (2025)

---

## ğŸ¤ Contributeurs

- Julien Forestier, Charles Auvrai, Quentin Chabot.

---

## ğŸ“ Support

Pour toute question technique, consultez :
- **Code source** : app.py (commentaires dÃ©taillÃ©s)
- **Logs de migration** : Onglet "Partie 2 : Migration"
- **Documentation MongoDB** : [docs.mongodb.com](https://docs.mongodb.com/)
- **Documentation Groq** : [console.groq.com/docs](https://console.groq.com/docs)
